{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T10:31:26.216288860Z",
     "start_time": "2023-12-12T10:31:26.209251348Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from PIL import ImageColor\n",
    "import matplotlib.colors\n",
    "\n",
    "from utils import NN_FEATURES_DIR, RESULTS_DIR, SUBJECTS, NUM_TEST_STIMULI\n",
    "from analyses.ridge_regression_decoding import NUM_CV_SPLITS, DECODER_OUT_DIR, calc_rsa, calc_rsa_images, calc_rsa_captions, get_fmri_data, pairwise_accuracy, ACC_CAPTIONS, ACC_IMAGES, get_default_features, get_default_vision_features, get_default_lang_features, MOD_SPECIFIC_IMAGES, MOD_SPECIFIC_CAPTIONS\n",
    "from notebook_utils import add_avg_subject, create_result_graph, plot_metric_catplot, plot_metric, load_results_data, ACC_MEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot cross-modal decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:00<00:00, 836.63it/s]\n"
     ]
    }
   ],
   "source": [
    "models = [\"random-flava\", \"clip\", \"flava\", \"imagebind\", \"blip2\"]\n",
    "all_data = load_results_data(models, recompute_acc_scores=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>model</th>\n",
       "      <th>subject</th>\n",
       "      <th>features</th>\n",
       "      <th>test_features</th>\n",
       "      <th>vision_features</th>\n",
       "      <th>lang_features</th>\n",
       "      <th>training_mode</th>\n",
       "      <th>mask</th>\n",
       "      <th>num_voxels</th>\n",
       "      <th>surface</th>\n",
       "      <th>resolution</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>model_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>blip2</td>\n",
       "      <td>sub-01</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>modality-agnostic</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_modality_agnostic</td>\n",
       "      <td>0.875983</td>\n",
       "      <td>blip2_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>blip2</td>\n",
       "      <td>sub-01</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>modality-agnostic</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_captions</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>blip2_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>blip2</td>\n",
       "      <td>sub-01</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>modality-agnostic</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_images</td>\n",
       "      <td>0.944099</td>\n",
       "      <td>blip2_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>blip2</td>\n",
       "      <td>sub-01</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>modality-agnostic</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_cross_images_to_captions</td>\n",
       "      <td>0.944513</td>\n",
       "      <td>blip2_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>blip2</td>\n",
       "      <td>sub-01</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>modality-agnostic</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_cross_captions_to_images</td>\n",
       "      <td>0.847205</td>\n",
       "      <td>blip2_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>random-flava</td>\n",
       "      <td>sub-07</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>images</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>123559</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_cross_images_to_captions</td>\n",
       "      <td>0.697516</td>\n",
       "      <td>random-flava_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>random-flava</td>\n",
       "      <td>sub-07</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>images</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>123559</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_cross_captions_to_images</td>\n",
       "      <td>0.537681</td>\n",
       "      <td>random-flava_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>random-flava</td>\n",
       "      <td>sub-07</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>images</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>123559</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_imagery</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>random-flava_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>random-flava</td>\n",
       "      <td>sub-07</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>images</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>123559</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_imagery_whole_test_set</td>\n",
       "      <td>0.755869</td>\n",
       "      <td>random-flava_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>random-flava</td>\n",
       "      <td>sub-07</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>vision_features_mean</td>\n",
       "      <td>lang_features_mean</td>\n",
       "      <td>images</td>\n",
       "      <td>whole_brain</td>\n",
       "      <td>123559</td>\n",
       "      <td>False</td>\n",
       "      <td>fsaverage</td>\n",
       "      <td>pairwise_acc_mean</td>\n",
       "      <td>0.617598</td>\n",
       "      <td>random-flava_avg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha         model subject features test_features  \\\n",
       "0     100000.0         blip2  sub-01      avg           avg   \n",
       "1     100000.0         blip2  sub-01      avg           avg   \n",
       "2     100000.0         blip2  sub-01      avg           avg   \n",
       "3     100000.0         blip2  sub-01      avg           avg   \n",
       "4     100000.0         blip2  sub-01      avg           avg   \n",
       "..         ...           ...     ...      ...           ...   \n",
       "715  1000000.0  random-flava  sub-07      avg           avg   \n",
       "716  1000000.0  random-flava  sub-07      avg           avg   \n",
       "717  1000000.0  random-flava  sub-07      avg           avg   \n",
       "718  1000000.0  random-flava  sub-07      avg           avg   \n",
       "719  1000000.0  random-flava  sub-07      avg           avg   \n",
       "\n",
       "          vision_features       lang_features      training_mode         mask  \\\n",
       "0    vision_features_mean  lang_features_mean  modality-agnostic  whole_brain   \n",
       "1    vision_features_mean  lang_features_mean  modality-agnostic  whole_brain   \n",
       "2    vision_features_mean  lang_features_mean  modality-agnostic  whole_brain   \n",
       "3    vision_features_mean  lang_features_mean  modality-agnostic  whole_brain   \n",
       "4    vision_features_mean  lang_features_mean  modality-agnostic  whole_brain   \n",
       "..                    ...                 ...                ...          ...   \n",
       "715  vision_features_mean  lang_features_mean             images  whole_brain   \n",
       "716  vision_features_mean  lang_features_mean             images  whole_brain   \n",
       "717  vision_features_mean  lang_features_mean             images  whole_brain   \n",
       "718  vision_features_mean  lang_features_mean             images  whole_brain   \n",
       "719  vision_features_mean  lang_features_mean             images  whole_brain   \n",
       "\n",
       "     num_voxels  surface resolution                                 metric  \\\n",
       "0        132633    False  fsaverage         pairwise_acc_modality_agnostic   \n",
       "1        132633    False  fsaverage                  pairwise_acc_captions   \n",
       "2        132633    False  fsaverage                    pairwise_acc_images   \n",
       "3        132633    False  fsaverage  pairwise_acc_cross_images_to_captions   \n",
       "4        132633    False  fsaverage  pairwise_acc_cross_captions_to_images   \n",
       "..          ...      ...        ...                                    ...   \n",
       "715      123559    False  fsaverage  pairwise_acc_cross_images_to_captions   \n",
       "716      123559    False  fsaverage  pairwise_acc_cross_captions_to_images   \n",
       "717      123559    False  fsaverage                   pairwise_acc_imagery   \n",
       "718      123559    False  fsaverage    pairwise_acc_imagery_whole_test_set   \n",
       "719      123559    False  fsaverage                      pairwise_acc_mean   \n",
       "\n",
       "        value        model_feat  \n",
       "0    0.875983         blip2_avg  \n",
       "1    0.847619         blip2_avg  \n",
       "2    0.944099         blip2_avg  \n",
       "3    0.944513         blip2_avg  \n",
       "4    0.847205         blip2_avg  \n",
       "..        ...               ...  \n",
       "715  0.697516  random-flava_avg  \n",
       "716  0.537681  random-flava_avg  \n",
       "717  1.000000  random-flava_avg  \n",
       "718  0.755869  random-flava_avg  \n",
       "719  0.617598  random-flava_avg  \n",
       "\n",
       "[720 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_models = all_data.copy()\n",
    "\n",
    "data_models = data_models[data_models[\"mask\"] == \"whole_brain\"]\n",
    "data_models = data_models[data_models.surface == False]\n",
    "\n",
    "data_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>model</th>\n",
       "      <th>subject</th>\n",
       "      <th>features</th>\n",
       "      <th>test_features</th>\n",
       "      <th>vision_features</th>\n",
       "      <th>lang_features</th>\n",
       "      <th>training_mode</th>\n",
       "      <th>mask</th>\n",
       "      <th>num_voxels</th>\n",
       "      <th>surface</th>\n",
       "      <th>resolution</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>model_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [alpha, model, subject, features, test_features, vision_features, lang_features, training_mode, mask, num_voxels, surface, resolution, metric, value, model_feat]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cls_feats = data_models.copy()\n",
    "for model in all_data.model.unique():\n",
    "    data_cls_feats = data_cls_feats[((data_cls_feats.model == model) & (data_cls_feats.vision_features == \"vision_features_cls\") & (data_cls_feats.lang_features == \"lang_features_cls\")) | (data_cls_feats.model != model)]\n",
    "    \n",
    "data_matched_feats = data_cls_feats[data_cls_feats.features == \"matched\"]\n",
    "data_matched_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mean_cross_modal_and_within_modal_rows(data):\n",
    "    extra_rows = []\n",
    "    for model in data.model.unique():\n",
    "        for mask in data[\"mask\"].unique():\n",
    "            for subject in SUBJECTS:\n",
    "                data_model_subj = data[(data.model == model) & (data.subject == subject)]\n",
    "                if pd.isna(mask):\n",
    "                    data_model_subj = data_model_subj[pd.isna(data_model_subj['mask'])]\n",
    "                else:\n",
    "                    data_model_subj = data_model_subj[data_model_subj['mask'] == mask]\n",
    "                # cross-modal\n",
    "                cross_modal_train_images_eval_captions = data_model_subj[(data_model_subj.training_mode == \"images\") & (data_model_subj.metric == ACC_CAPTIONS)]\n",
    "                cross_modal_train_captions_eval_images = data_model_subj[(data_model_subj.training_mode == \"captions\") & (data_model_subj.metric == ACC_IMAGES)]\n",
    "        \n",
    "                if len(cross_modal_train_captions_eval_images) > 0:\n",
    "                    assert len(cross_modal_train_images_eval_captions) == len(cross_modal_train_captions_eval_images) == 1\n",
    "                    mean_acc = (cross_modal_train_images_eval_captions.value.item() + cross_modal_train_captions_eval_images.value.item()) / 2\n",
    "            \n",
    "                    mean_row = cross_modal_train_images_eval_captions.copy()\n",
    "                    mean_row[\"training_mode\"] = \"cross-modal\"\n",
    "                    mean_row[\"metric\"] = \"mean\"\n",
    "                    mean_row[\"value\"] = mean_acc\n",
    "                    mean_row[\"condition\"] = \"cross-modal\"\n",
    "        \n",
    "                    extra_rows.append(mean_row)\n",
    "        \n",
    "                # within-modal\n",
    "                within_modal_captions = data_model_subj[(data_model_subj.training_mode == \"captions\") & (data_model_subj.metric == ACC_CAPTIONS)]\n",
    "                within_modal_images = data_model_subj[(data_model_subj.training_mode == \"images\") & (data_model_subj.metric == ACC_IMAGES)]\n",
    "        \n",
    "                if len(within_modal_captions) > 0:\n",
    "                    assert len(within_modal_captions) == len(within_modal_images) == 1\n",
    "                    mean_acc = (within_modal_captions.value.item() + within_modal_images.value.item()) / 2\n",
    "            \n",
    "                    mean_row = within_modal_captions.copy()\n",
    "                    mean_row[\"training_mode\"] = \"within-modal\"\n",
    "                    mean_row[\"metric\"] = \"mean\"\n",
    "                    mean_row[\"value\"] = mean_acc\n",
    "                    mean_row[\"condition\"] = \"within-modal\"\n",
    "        \n",
    "                    extra_rows.append(mean_row)\n",
    "\n",
    "    if len(extra_rows) > 0:\n",
    "        extra_rows = pd.concat(extra_rows)\n",
    "        data = pd.concat((data, extra_rows), ignore_index=True)\n",
    "    return data\n",
    "    # data_matched_with_mean[data_matched_with_mean.metric == \"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matched_feats = add_mean_cross_modal_and_within_modal_rows(data_matched_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FEAT_OPTIONS = [\"vision\", \"lang\", \"matched\"]\n",
    "\n",
    "def calc_model_feat_order(data, model_order, feat_options=DEFAULT_FEAT_OPTIONS):\n",
    "    all_model_feats = data.model_feat.unique()\n",
    "    all_models = data.model.unique()\n",
    "    for model in all_models:\n",
    "        if model not in model_order:\n",
    "            raise RuntimeError(f\"Model missing in order: {model}\")\n",
    "    model_feat_order = []\n",
    "    for model in model_order:\n",
    "        for feats in feat_options:\n",
    "            model_feat = f\"{model}_{feats}\"\n",
    "            if model_feat in all_model_feats:\n",
    "                model_feat_order.append(model_feat)\n",
    "\n",
    "    return model_feat_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_cross_modal_plot(data, x_variable, ylim=(0.5, 1), y_variable=\"value\", ylabel=\"pairwise_acc\", row_variable=\"metric\", hue_variable=\"condition\", title=None):\n",
    "    sns.set(font_scale=1.6)\n",
    "    TRAIN_MODE_ORDER = [\"images\", \"captions\", \"modality-agnostic\"]\n",
    "    FEAT_ORDER = [\"vision models\", \"language models\", \"multimodal models\"]\n",
    "    \n",
    "    data_to_plot = data.copy()\n",
    "    \n",
    "    data_to_plot = data_to_plot[data_to_plot.training_mode != \"modality-agnostic\"]\n",
    "    \n",
    "    data_to_plot[\"features\"] = data_to_plot.features.replace({\"vision\": \"vision models\", \"lang\": \"language models\", \"matched\": \"multimodal models\"})\n",
    "    \n",
    "    data_to_plot.loc[((data_to_plot.training_mode == \"images\") & (data_to_plot.metric == ACC_CAPTIONS)) | ((data_to_plot.training_mode == \"captions\") & (data_to_plot.metric == ACC_IMAGES)), \"condition\"] = \"cross-modal\"\n",
    "    data_to_plot.loc[((data_to_plot.training_mode == \"captions\") & (data_to_plot.metric == ACC_CAPTIONS)) | ((data_to_plot.training_mode == \"images\") & (data_to_plot.metric == ACC_IMAGES)), \"condition\"] = \"within-modal\"\n",
    "    \n",
    "    # model_feat_order = calc_model_feat_order(data_to_plot, model_order)\n",
    "    \n",
    "    \n",
    "    metrics_order = [ACC_CAPTIONS, ACC_IMAGES, \"mean\"]\n",
    "    \n",
    "    height = 4.5\n",
    "    aspect = 4\n",
    "    \n",
    "    condition_order = [\"cross-modal\", \"within-modal\"]\n",
    "    \n",
    "    for mode in [\"captions\", \"images\", \"cross-modal\", \"within-modal\"]:\n",
    "        data_mode = data_to_plot[data_to_plot.training_mode == mode]\n",
    "        for x_variable_value in data_to_plot[x_variable].unique():\n",
    "            for condition in condition_order:\n",
    "                length = len(data_mode[(data_mode[x_variable] == x_variable_value) & (data_mode.condition == condition)])\n",
    "                expected_num_datapoints = len(SUBJECTS)\n",
    "                if (length > 0) and (length != expected_num_datapoints):\n",
    "                    message = f\"unexpected number of datapoints: {length} (expected: {expected_num_datapoints}) (model_feat: {model} {mode})\"\n",
    "                    print(f\"Warning: {message}\")\n",
    "    \n",
    "    g = sns.catplot(data_to_plot, kind=\"bar\", x=x_variable, y=y_variable, row=row_variable, row_order=metrics_order, col=None, height=height, aspect=aspect, hue=hue_variable, hue_order=condition_order,\n",
    "                    palette=None, err_kws={'linewidth': 0.5, 'alpha': 0.99}, width=0.7)\n",
    "\n",
    "    g.set(ylim=ylim, ylabel=ylabel, xlabel='')\n",
    "    g.tick_params(axis='x', rotation=80)\n",
    "    if title:\n",
    "        g.fig.suptitle(title, y=1.03)\n",
    "    return data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a frame with no defined index and a scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_plotted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_zero_shot_cross_modal_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_matched_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mylim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RESULTS_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_shot_cross_modal.png\u001b[39m\u001b[38;5;124m\"\u001b[39m), bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m, pad_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_plotted[(data_plotted\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mcreate_zero_shot_cross_modal_plot\u001b[0;34m(data, x_variable, ylim, y_variable, ylabel, row_variable, hue_variable, title)\u001b[0m\n\u001b[1;32m      8\u001b[0m data_to_plot \u001b[38;5;241m=\u001b[39m data_to_plot[data_to_plot\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodality-agnostic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m data_to_plot[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_to_plot\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision models\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage models\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatched\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultimodal models\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdata_to_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_to_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_to_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mACC_CAPTIONS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_to_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcaptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_to_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mACC_IMAGES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcondition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross-modal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m data_to_plot\u001b[38;5;241m.\u001b[39mloc[((data_to_plot\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptions\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (data_to_plot\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m ACC_CAPTIONS)) \u001b[38;5;241m|\u001b[39m ((data_to_plot\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (data_to_plot\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m ACC_IMAGES)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithin-modal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# model_feat_order = calc_model_feat_order(data_to_plot, model_order)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/pandas/core/indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 885\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/pandas/core/indexing.py:1809\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like_indexer(value):\n\u001b[0;32m-> 1809\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1810\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a frame with no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1811\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefined index and a scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1812\u001b[0m         )\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a frame with no defined index and a scalar"
     ]
    }
   ],
   "source": [
    "data_plotted = create_zero_shot_cross_modal_plot(data_matched_feats, \"model\", ylim=(0.3, 1))\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"zero_shot_cross_modal.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "\n",
    "print(data_plotted[(data_plotted.metric == \"mean\")].groupby([\"model\", \"condition\"])['value'].mean())\n",
    "print(data_plotted[(data_plotted.metric == \"mean\")].groupby([\"model\", \"condition\"])['value'].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot cross-modal decoding with Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_cross_modal_masks_plot(model):\n",
    "    model_order = [model]\n",
    "    resolution = \"fsaverage7\"\n",
    "    \n",
    "    data_models = all_data[all_data.model.isin(model_order)].copy()\n",
    "    \n",
    "    include_masks = [mask_name for mask_name in data_models[\"mask\"].unique() if not \"masks_400\" in mask_name and \"imagebind\" in mask_name] #\"thresh_0.001\" in mask_name and \n",
    "    include_masks += [\"whole_brain\"]\n",
    "    data_models = data_models[data_models[\"mask\"].isin(include_masks)]\n",
    "    # print(data_models[\"mask\"].unique())\n",
    "\n",
    "    data_models[\"mask\"] = data_models[\"mask\"].apply(lambda x: os.path.basename(x)) #\n",
    "\n",
    "    \n",
    "    # data_models = data_models[data_models.resolution == resolution]\n",
    "    # data_models = data_models[data_models.surface == True]\n",
    "    \n",
    "    data_cls_feats = data_models.copy()\n",
    "    for model in data_models.model.unique():\n",
    "        data_cls_feats = data_cls_feats[((data_cls_feats.model == model) & (data_cls_feats.vision_features == \"vision_features_cls\") & (data_cls_feats.lang_features == \"lang_features_cls\")) | (data_cls_feats.model != model)]\n",
    "    \n",
    "    data_matched_feats = data_cls_feats[data_cls_feats.features == \"matched\"]\n",
    "    \n",
    "    data_matched_feats = add_mean_cross_modal_and_within_modal_rows(data_matched_feats)\n",
    "    data_plotted = create_zero_shot_cross_modal_plot(data_matched_feats, x_variable=\"mask\", title=model)\n",
    "    # print(data_plotted[(data_plotted.metric == \"mean\")].groupby([\"mask\", \"condition\"]).agg(mean_val=('value', 'mean'), count=('value', 'count')))\n",
    "    return data_plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plotted = create_zero_shot_cross_modal_masks_plot(\"blip2\")\n",
    "create_zero_shot_cross_modal_masks_plot(\"imagebind\")\n",
    "create_zero_shot_cross_modal_masks_plot(\"clip\")\n",
    "create_zero_shot_cross_modal_masks_plot(\"flava\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plotted = data_plotted[data_plotted[\"mask\"] != \"whole_brain\"]\n",
    "data_plotted = create_zero_shot_cross_modal_plot(data_plotted, x_variable=\"mask\", y_variable=\"num_voxels\", ylim=(0, 2000), ylabel=\"num_vertices\", row_variable=None, hue_variable=None)\n",
    "_ = plt.title(\"Mask sizes\")\n",
    "# print(data_plotted[(data_plotted.metric == \"mean\")].groupby([\"mask\", \"condition\"])['num_voxels'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot cross-modal decoding with GloW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"glow\", \"glow-contrastive\"]\n",
    "\n",
    "data_models = load_results_data(models)\n",
    "data_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls_feats = data_models.copy()\n",
    "# for model in all_data.model.unique():\n",
    "#     data_cls_feats = data_cls_feats[((data_cls_feats.model == model) & (data_cls_feats.vision_features == \"vision_features_cls\") & (data_cls_feats.lang_features == \"lang_features_cls\")) | (data_cls_feats.model != model)]\n",
    "\n",
    "data_matched_feats = data_cls_feats[data_cls_feats.features == \"matched\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matched_feats = add_mean_cross_modal_and_within_modal_rows(data_matched_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plotted = create_zero_shot_cross_modal_plot(data_matched_feats, \"model\")\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"zero_shot_cross_modal_glow.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "\n",
    "print(data_plotted[(data_plotted.training_mode == \"cross-modal\") & (data_plotted.metric == \"mean\")].groupby(\"model\")['value'].mean())\n",
    "print(data_plotted[(data_plotted.training_mode == \"cross-modal\") & (data_plotted.metric == \"mean\")].groupby(\"model\")['value'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "64c2261fd1335a391d209058834a77a3cc43bbc1dadc63860e2129a303b1f182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
