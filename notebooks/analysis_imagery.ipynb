{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from PIL import ImageColor\n",
    "import matplotlib.colors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import LATENT_FEATURES_DIR, RESULTS_DIR, SUBJECTS, FMRI_BETAS_SURFACE_DIR, STIM_INFO_PATH, COCO_IMAGES_DIR, METRIC_DIFF_MOD_AGNOSTIC_MOD_SPECIFIC, METRIC_CROSS_DECODING, FMRI_DATA_DIR, DECODER_ADDITIONAL_TEST_OUT_DIR, FMRI_BIDS_DATA_DIR\n",
    "from analyses.decoding.ridge_regression_decoding import NUM_CV_SPLITS, pairwise_accuracy, get_run_str, RESULTS_FILE, PREDICTIONS_FILE\n",
    "from data import MODALITY_AGNOSTIC, MODALITY_SPECIFIC_IMAGES, MODALITY_SPECIFIC_CAPTIONS, TRAINING_MODES, CAPTION, IMAGE, TEST_SPLITS, LatentFeatsConfig, get_stim_info, SPLIT_TRAIN, SPLIT_TEST_IMAGES, SPLIT_TEST_CAPTIONS, SPLIT_IMAGERY, SPLIT_IMAGERY_WEAK, get_latents_for_splits, standardize_latents, IMAGERY\n",
    "from eval import ACC_MODALITY_AGNOSTIC, ACC_CAPTIONS, ACC_IMAGES, ACC_CROSS_IMAGES_TO_CAPTIONS, ACC_CROSS_CAPTIONS_TO_IMAGES, ACC_IMAGERY, ACC_IMAGERY_WHOLE_TEST, get_distance_matrix\n",
    "from scipy.stats import ttest_rel\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from feature_extraction.feat_extraction_utils import CoCoDataset\n",
    "from scipy.spatial.distance import cdist\n",
    "from pdf2image import convert_from_path\n",
    "from notebook_utils import load_predictions, load_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_ds = CoCoDataset(COCO_IMAGES_DIR, STIM_INFO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(image, length=100):\n",
    "    if image.size[0] < image.size[1]:\n",
    "        resized_image = image.resize((length, int(image.size[1] * (length / image.size[0]))))\n",
    "        required_loss = (resized_image.size[1] - length)\n",
    "        resized_image = resized_image.crop(box=(0, required_loss / 2, length, resized_image.size[1] - required_loss / 2))\n",
    "    else:\n",
    "        resized_image = image.resize((int(image.size[0] * (length / image.size[1])), length))\n",
    "        required_loss = resized_image.size[0] - length\n",
    "        resized_image = resized_image.crop(box=(required_loss / 2, 0, resized_image.size[0] - required_loss / 2, length))\n",
    "    return resized_image\n",
    "\n",
    "def display_stimuli(coco_ids, imgs=True, caps=True):\n",
    "    if caps:\n",
    "        for coco_id in coco_ids:\n",
    "            print(coco_ds.captions[coco_id], end=\"\\n\")\n",
    "\n",
    "    if imgs:\n",
    "        imgs = [np.array(resize_img(coco_ds.get_img_by_coco_id(img_id))) for img_id in coco_ids]        \n",
    "        img = Image.fromarray(np.hstack(imgs))\n",
    "        display(img)\n",
    "\n",
    "def get_distance_matrix(predictions, originals, metric='cosine'):\n",
    "    dist = cdist(predictions, originals, metric=metric)\n",
    "    return dist\n",
    "    \n",
    "def dist_mat_to_pairwise_acc(dist_mat, stim_ids, print_details=False):\n",
    "    diag = dist_mat.diagonal().reshape(-1, 1)\n",
    "    comp_mat = diag < dist_mat\n",
    "    corrects = comp_mat.sum()\n",
    "    if print_details:\n",
    "        for i, stim_id in enumerate(stim_ids):\n",
    "            print(stim_id, end=': ')\n",
    "            print(f'{comp_mat[i].sum() / (len(comp_mat[i]) - 1):.2f}')\n",
    "    # subtract the number of elements of the diagonal as these values are always \"False\" (not smaller than themselves)\n",
    "    score = corrects / (dist_mat.size - diag.size)\n",
    "    return score\n",
    "\n",
    "def dist_mat_to_rankings(dist_mat, stim_ids, candidate_set_latent_ids):\n",
    "    all_ranks = []\n",
    "    for test_stimulus_id, nneighbors_row in zip(stim_ids, dist_mat):\n",
    "        nneighbors_ids = np.array(candidate_set_latent_ids)[np.argsort(nneighbors_row)]\n",
    "        rank = np.argwhere(nneighbors_ids == test_stimulus_id)[0][0] + 1\n",
    "\n",
    "        all_ranks.append(rank)\n",
    "            \n",
    "    return np.mean(all_ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors of imagery images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 5\n",
    "N_SAMPLES = 5\n",
    "FONTSIZE = 13\n",
    "\n",
    "\n",
    "def plot_nn_table(stim_ids, nneighbors, subject, stim_type, out_file_name=None, img_length=150, hspace=0.2):\n",
    "    stimulus_key = 'Imagery sketch and initial\\ninstruction' if stim_type == IMAGERY else 'Stimulus'\n",
    "    figsize=(16, 11) if stim_type == IMAGERY else (16, 15)\n",
    "    \n",
    "    df = pd.DataFrame({stimulus_key: stim_ids} | {f'rank {i}': [n[i] for n in nneighbors] for i in range(len(nneighbors[0]))})\n",
    "\n",
    "    n_columns = len(nneighbors[0])+1#len(nneighbors[0])+2 if stim_type == IMAGERY else len(nneighbors[0])+1\n",
    "    fig, axes = plt.subplots(len(stim_ids),n_columns, figsize=figsize) #, layout=\"constrained\"\n",
    "\n",
    "    if stim_type == IMAGERY:\n",
    "        fig.subplots_adjust(wspace=0.05, hspace=hspace, top=0.97, bottom=0.06, left=0.01, right=0.99)   \n",
    "    else:\n",
    "        fig.subplots_adjust(wspace=0.05, hspace=hspace, top=0.98, bottom=0.03, left=0.01, right=0.99)  \n",
    "       \n",
    "\n",
    "    for idx, (stim_id, neighbors) in enumerate(zip(stim_ids, nneighbors)):\n",
    "        caption = coco_ds.captions[stim_id].lower()\n",
    "        img = resize_img(coco_ds.get_img_by_coco_id(stim_id), length=img_length)\n",
    "\n",
    "        if stim_type == IMAGE:\n",
    "            axes[idx][0].imshow(img)\n",
    "        elif stim_type == CAPTION:\n",
    "            img = Image.fromarray(np.full((img_length, img_length, 3), 255, dtype=np.uint8), \"RGB\")\n",
    "            axes[idx][0].imshow(img)\n",
    "            txt = axes[idx][0].text(0, img_length/2, caption, ha='left', wrap=True, fontsize=FONTSIZE,# verticalalignment='top', \n",
    "                                   bbox=dict(boxstyle='square,pad=0', facecolor='none', edgecolor='none'))\n",
    "            txt._get_wrap_line_width = lambda : img_length*4\n",
    "        elif stim_type == IMAGERY:\n",
    "            drawing_path = os.path.join(FMRI_BIDS_DATA_DIR, \"stimuli\", \"imagery_drawings\", f\"{subject}_imagery_{idx+1}.pdf\")\n",
    "            img_drawing = convert_from_path(drawing_path)[0]\n",
    "            img_drawing = resize_img(img_drawing, length=img_length)\n",
    "            \n",
    "            axes[idx][0].imshow(img_drawing)\n",
    "            txt = axes[idx][0].text(0, 155, caption, ha='left', wrap=True, fontsize=FONTSIZE, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='square,pad=0', facecolor='none', edgecolor='none'))\n",
    "            txt._get_wrap_line_width = lambda : img_length*4\n",
    "            # axes[idx][1].axis('off')\n",
    "            \n",
    "        axes[idx][0].axis('off')\n",
    "        \n",
    "        if idx == 0:\n",
    "            axes[idx][0].set_title(f'{stimulus_key}', fontweight=\"bold\")\n",
    "    \n",
    "\n",
    "        for n_id, neighbor_id in enumerate(neighbors):\n",
    "            caption = coco_ds.captions[neighbor_id].lower()\n",
    "            img = resize_img(coco_ds.get_img_by_coco_id(neighbor_id), length=img_length)\n",
    "            axes[idx][n_id+1].imshow(img)\n",
    "            axes[idx][n_id+1].axis('off')\n",
    "            if idx == 0:\n",
    "                axes[idx][n_id+1].set_title(f'Rank {n_id}', fontweight=\"bold\")\n",
    "\n",
    "            txt = axes[idx][n_id+1].text(0, 155, caption, ha='left', wrap=True, fontsize=12, verticalalignment='top',\n",
    "                                   bbox=dict(boxstyle='square,pad=0', facecolor='none', edgecolor='none'))\n",
    "            if stim_type == IMAGERY:\n",
    "                txt._get_wrap_line_width = lambda : img_length*4\n",
    "            else:\n",
    "                txt._get_wrap_line_width = lambda : img_length*4\n",
    "\n",
    " \n",
    "    if out_file_name is not None:\n",
    "        out_path = os.path.join(RESULTS_DIR, \"analysis_ranking\", out_file_name)\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        plt.savefig(out_path, dpi=250)\n",
    "\n",
    "\n",
    "def analysis_ranking(test_preds, test_stim_ids, candidate_latents, candidate_latent_ids, subject, stim_type, n_samples=N_SAMPLES, num_neighbors=N_NEIGHBORS, out_file_name=None, hspace=0.2):\n",
    "    dist_mat = get_distance_matrix(test_preds, candidate_latents)\n",
    "  \n",
    "    acc = dist_mat_to_pairwise_acc(dist_mat, test_stim_ids)\n",
    "    print(f'pairwise acc: {acc:.3f}')\n",
    "\n",
    "    if stim_type != IMAGERY:\n",
    "        np.random.seed(7)\n",
    "        sampled_ids = np.random.choice(range(len(test_stim_ids)), n_samples, replace=False)\n",
    "        test_stim_ids = np.array(test_stim_ids)[sampled_ids]\n",
    "        dist_mat = dist_mat[sampled_ids]\n",
    "\n",
    "    nneighbors = [np.array(candidate_latent_ids)[np.argsort(nneighbors_row)][:num_neighbors] for nneighbors_row in dist_mat]\n",
    "\n",
    "    plot_nn_table(test_stim_ids, nneighbors, subject, stim_type, out_file_name, hspace=hspace)\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbors of imagery trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"imagebind\"\n",
    "TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "FEATS = 'default'\n",
    "TEST_FEATS = 'default'\n",
    "VISION_FEATS = 'default'\n",
    "LANG_FEATS = 'default'\n",
    "FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "all_train_stim_ids = []\n",
    "all_train_latents = []\n",
    "for subj in tqdm(SUBJECTS):\n",
    "    stim_ids, _ = get_stim_info(subj, SPLIT_TRAIN)\n",
    "    \n",
    "    latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN], TRAINING_MODE)\n",
    "    latents = standardize_latents(latents)\n",
    "\n",
    "    all_train_stim_ids.append(stim_ids)\n",
    "    all_train_latents.append(latents[SPLIT_TRAIN])\n",
    "\n",
    "all_train_stim_ids = np.concatenate(all_train_stim_ids)\n",
    "all_train_latents = np.concatenate(all_train_latents)\n",
    "\n",
    "unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "unique_train_latents = all_train_latents[indices]\n",
    "print(len(unique_stim_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 5\n",
    "N_SAMPLES = 3\n",
    "\n",
    "WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET = True\n",
    "SURFACE = True\n",
    "\n",
    "MODEL = \"imagebind\"\n",
    "\n",
    "SUBJECTS = ['sub-01', 'sub-02', 'sub-05', 'sub-07']\n",
    "\n",
    "# TRAINING_MODE = \"images\"\n",
    "# TRAINING_MODE = \"captions\"\n",
    "TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "MASK = None\n",
    "\n",
    "BETAS_SUFFIX = 'betas'\n",
    "\n",
    "BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "RESTANDARDIZE_PREDS = [SPLIT_IMAGERY]\n",
    "TRAINING_SPLITS = [SPLIT_TRAIN]\n",
    "IMAGERY_SAMPLES_WEIGHT = None\n",
    "\n",
    "FEATS = 'default'\n",
    "TEST_FEATS = 'default'\n",
    "VISION_FEATS = 'default'\n",
    "LANG_FEATS = 'default'\n",
    "FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "all_pairwise_accs = []\n",
    "for subj in SUBJECTS:\n",
    "    print(subj)\n",
    "\n",
    "    stim_ids_test, _ = get_stim_info(subj, SPLIT_TEST_IMAGES)\n",
    "    stim_ids_imagery, _ =  get_stim_info(subj, SPLIT_IMAGERY)\n",
    "\n",
    "    latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN, SPLIT_TEST_IMAGES, SPLIT_IMAGERY], TRAINING_MODE)\n",
    "    latents = standardize_latents(latents)\n",
    "\n",
    "    predictions = load_predictions(BETAS_DIR, subj, TRAINING_MODE, FEATS_CONFIG, surface=SURFACE, mask=MASK, training_splits=TRAINING_SPLITS, imagery_samples_weight=IMAGERY_SAMPLES_WEIGHT)\n",
    "\n",
    "    pred_latents_imagery = predictions[SPLIT_IMAGERY]\n",
    "    if len(RESTANDARDIZE_PREDS)>0:\n",
    "        print('standardizing imagery predictions')\n",
    "        refs = np.concatenate([predictions[split] for split in RESTANDARDIZE_PREDS])\n",
    "        print(len(refs))\n",
    "        transform = StandardScaler().fit(refs)\n",
    "        pred_latents_imagery = transform.transform(pred_latents_imagery)\n",
    "        # pred_latents_imagery = StandardScaler().fit_transform(pred_latents_imagery)\n",
    "\n",
    "    # test_stim_ids_mod = results['stimulus_ids'][results['stimulus_types'] == IMAGE]\n",
    "    # test_latents_mod = results['latents'][results['stimulus_types'] == IMAGE]\n",
    "\n",
    "    if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "        # # account for case that sometimes both the image and the caption are part of the training set\n",
    "        # unique_stim_ids, indices = np.unique(stim_ids, return_index=True)\n",
    "        # unique_train_latents = train_latents[indices]\n",
    "        unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "        unique_train_latents = all_train_latents[indices]\n",
    "        print('candidate set size: ', len(unique_stim_ids))\n",
    "\n",
    "        candidate_latents = np.concatenate((latents[SPLIT_IMAGERY], latents[SPLIT_TEST_IMAGES], unique_train_latents))\n",
    "        candidate_latent_ids = np.concatenate((stim_ids_imagery, stim_ids_test, unique_stim_ids))\n",
    "    else:\n",
    "        candidate_latents = latents[SPLIT_IMAGERY]\n",
    "        candidate_latent_ids = stim_ids_imagery\n",
    "\n",
    "    acc = analysis_ranking(pred_latents_imagery, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, IMAGERY, N_SAMPLES, N_NEIGHBORS, out_file_name=f\"{IMAGERY}_{TRAINING_MODE}_decoder_{subj}.png\", hspace=0.2)\n",
    "    all_pairwise_accs.append(acc)\n",
    "\n",
    "print(f'Mean pairwise acc: {np.mean(all_pairwise_accs):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 5\n",
    "N_SAMPLES = 3\n",
    "\n",
    "WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET = True\n",
    "SURFACE = True\n",
    "\n",
    "MODEL = \"imagebind\"\n",
    "\n",
    "SUBJECTS = ['sub-01', 'sub-02', 'sub-05', 'sub-07']\n",
    "\n",
    "# TRAINING_MODE = \"images\"\n",
    "# TRAINING_MODE = \"captions\"\n",
    "TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "MASK = None\n",
    "\n",
    "BETAS_SUFFIX = 'betas'\n",
    "\n",
    "BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "RESTANDARDIZE_PREDS = [SPLIT_IMAGERY]\n",
    "TRAINING_SPLITS = [SPLIT_TRAIN, SPLIT_IMAGERY_WEAK]\n",
    "IMAGERY_SAMPLES_WEIGHT = 500\n",
    "\n",
    "FEATS = 'default'\n",
    "TEST_FEATS = 'default'\n",
    "VISION_FEATS = 'default'\n",
    "LANG_FEATS = 'default'\n",
    "FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "all_pairwise_accs = []\n",
    "for subj in SUBJECTS:\n",
    "    print(subj)\n",
    "\n",
    "    stim_ids_test, _ = get_stim_info(subj, SPLIT_TEST_IMAGES)\n",
    "    stim_ids_imagery, _ =  get_stim_info(subj, SPLIT_IMAGERY)\n",
    "\n",
    "    latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN, SPLIT_TEST_IMAGES, SPLIT_IMAGERY], TRAINING_MODE)\n",
    "    latents = standardize_latents(latents)\n",
    "\n",
    "    predictions = load_predictions(BETAS_DIR, subj, TRAINING_MODE, FEATS_CONFIG, surface=SURFACE, mask=MASK, training_splits=TRAINING_SPLITS, imagery_samples_weight=IMAGERY_SAMPLES_WEIGHT)\n",
    "\n",
    "    pred_latents_imagery = predictions[SPLIT_IMAGERY]\n",
    "    if len(RESTANDARDIZE_PREDS)>0:\n",
    "        print('standardizing imagery predictions')\n",
    "        refs = np.concatenate([predictions[split] for split in RESTANDARDIZE_PREDS])\n",
    "        print(len(refs))\n",
    "        transform = StandardScaler().fit(refs)\n",
    "        pred_latents_imagery = transform.transform(pred_latents_imagery)\n",
    "        # pred_latents_imagery = StandardScaler().fit_transform(pred_latents_imagery)\n",
    "\n",
    "    # test_stim_ids_mod = results['stimulus_ids'][results['stimulus_types'] == IMAGE]\n",
    "    # test_latents_mod = results['latents'][results['stimulus_types'] == IMAGE]\n",
    "\n",
    "    if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "        # # account for case that sometimes both the image and the caption are part of the training set\n",
    "        # unique_stim_ids, indices = np.unique(stim_ids, return_index=True)\n",
    "        # unique_train_latents = train_latents[indices]\n",
    "        unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "        unique_train_latents = all_train_latents[indices]\n",
    "        print('candidate set size: ', len(unique_stim_ids))\n",
    "\n",
    "        candidate_latents = np.concatenate((latents[SPLIT_IMAGERY], latents[SPLIT_TEST_IMAGES], unique_train_latents))\n",
    "        candidate_latent_ids = np.concatenate((stim_ids_imagery, stim_ids_test, unique_stim_ids))\n",
    "    else:\n",
    "        candidate_latents = latents[SPLIT_IMAGERY]\n",
    "        candidate_latent_ids = stim_ids_imagery\n",
    "\n",
    "    acc = analysis_ranking(pred_latents_imagery, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, IMAGERY, N_SAMPLES, N_NEIGHBORS, out_file_name=f\"{IMAGERY}_{TRAINING_MODE}_decoder_{subj}_train_with_imagery_samples_weight_{IMAGERY_SAMPLES_WEIGHT}.png\", hspace=0.2)\n",
    "    all_pairwise_accs.append(acc)\n",
    "\n",
    "print(f'Mean pairwise acc: {np.mean(all_pairwise_accs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE for Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fmri_betas_full, train_stim_ids, train_stim_types = get_fmri_data(\n",
    "#     BETAS_DIR,\n",
    "#     SUBJECT,\n",
    "#     SPLIT_TRAIN,\n",
    "#     TRAINING_MODE,\n",
    "#     surface=SURFACE,\n",
    "# )\n",
    "# N_TRAIN_BETAS = 1000\n",
    "# train_paths = train_paths[np.random.choice(range(len(train_paths)), size=N_TRAIN_BETAS, replace=False)]\n",
    "\n",
    "# train_fmri_betas, test_fmri_betas, train_fmri_betas_standardized, test_fmri_betas_standardized = load_betas(train_paths, test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_betas(train_betas, test_betas, title, binwidth=3):\n",
    "#     X = np.concatenate((train_betas.flatten(), test_betas.flatten()))\n",
    "#     hue = ['train'] * train_betas.size + ['test'] * test_betas.size\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     sns.histplot(x=X, hue=hue, binwidth=binwidth)\n",
    "#     plt.title(title)\n",
    "\n",
    "# print(np.nanmean(train_fmri_betas.mean(axis=0)))\n",
    "# print(np.nanmean(test_fmri_betas.mean(axis=0)))\n",
    "# print(np.nanmean(train_fmri_betas_standardized.mean(axis=0)))\n",
    "# print(np.nanmean(test_fmri_betas_standardized.mean(axis=0)))\n",
    "\n",
    "# plot_betas(train_fmri_betas, test_fmri_betas, title='unstandardized')\n",
    "# plt.ylim(0, 10000000)\n",
    "# plt.xlim(-25, 25)\n",
    "\n",
    "# plot_betas(train_fmri_betas_standardized, test_fmri_betas, title='standardized', binwidth=0.3)\n",
    "# plt.ylim(0, 4000000)\n",
    "# plt.xlim(-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBJECT = 'sub-01'\n",
    "# MODEL = \"imagebind\"\n",
    "# SURFACE = True\n",
    "\n",
    "# # TRAINING_MODE = \"images\"\n",
    "# TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "# BETAS_SUFFIX = 'betas'\n",
    "# BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "# # feats = 'avg'\n",
    "# # test_feats = 'avg'\n",
    "# FEATS = 'default'\n",
    "# TEST_FEATS = 'default'\n",
    "# # feats = 'lang'\n",
    "# # test_feats = 'lang'\n",
    "# # vision_feats = 'vision_features_cls'\n",
    "# VISION_FEATS = 'default'\n",
    "# # vision_feats = 'n_a'\n",
    "\n",
    "# LANG_FEATS = 'default'\n",
    "# # lang_feats = 'lang_features_cls'\n",
    "# # lang_feats = 'lang_features_mean'\n",
    "\n",
    "# FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "\n",
    "# test_fmri_betas, test_stim_ids, test_stim_types = get_fmri_data(\n",
    "#     BETAS_DIR,\n",
    "#     SUBJECT,\n",
    "#     SPLIT_TEST,\n",
    "#     surface=SURFACE,\n",
    "# )\n",
    "# imagery_fmri_betas, imagery_stim_ids, imagery_stim_types = get_fmri_data(\n",
    "#     BETAS_DIR,\n",
    "#     SUBJECT,\n",
    "#     SPLIT_IMAGERY,\n",
    "#     surface=SURFACE,\n",
    "# )\n",
    "\n",
    "# train_latents = get_latent_features(FEATS_CONFIG, SUBJECT, SPLIT_TRAIN, mode=TRAINING_MODE)\n",
    "# test_latents = get_latent_features(FEATS_CONFIG, SUBJECT, SPLIT_TEST)\n",
    "# imagery_latents = get_latent_features(FEATS_CONFIG, SUBJECT, SPLIT_IMAGERY)\n",
    "\n",
    "# train_latents, test_latents, imagery_latents = standardize_latents(\n",
    "#     train_latents, test_latents, imagery_latents\n",
    "# )\n",
    "\n",
    "# results = load_predictions(BETAS_DIR, SUBJECT, TRAINING_MODE, FEATS_CONFIG, surface=SURFACE)\n",
    "\n",
    "# # stim_ids, stim_types, train_latents, gray_matter_mask, results, train_paths, test_paths = load(BETAS_DIR, MODEL, SUBJECT, MODE, FEATS_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_latents_tsne(train_lat, test_lat, pred_lat, imagery_pred_lat, title, train_subset=1000):\n",
    "#     train_latents_subset = train_latents[np.random.choice(range(len(train_lat)), size=train_subset, replace=False)]\n",
    "    \n",
    "#     tsne = TSNE(n_components=2, learning_rate='auto', verbose=1, n_jobs=10, n_iter=1000)\n",
    "#     X_embedded = tsne.fit_transform(np.concatenate((train_latents_subset, test_lat, pred_lat, imagery_pred_lat)))\n",
    "    \n",
    "#     print(X_embedded.shape)\n",
    "#     assert X_embedded.shape[1] == 2\n",
    "#     hue = ['train'] * len(train_latents_subset) + ['test'] * len(test_lat) + ['predictions'] * len(pred_lat) + ['imagery_predictions'] * len(imagery_pred_lat)\n",
    "#     # alphas = [0.3] * len(train_latents_subset) + [1] * len(test_lat) + [1] * len(preds)\n",
    "    \n",
    "#     plt.figure(figsize=(20, 12))\n",
    "#     sns.scatterplot(\n",
    "#         x = X_embedded[:, 0], y = X_embedded[:, 1],\n",
    "#         hue = hue,\n",
    "#         alpha = 0.8\n",
    "#     )\n",
    "#     plt.title(title)\n",
    "\n",
    "# plot_latents_tsne(train_latents, results['latents'], results['predictions'], results['imagery_predictions'], title=\"not standardized\")\n",
    "\n",
    "# pred_latents_standardized = StandardScaler().fit_transform(results['predictions'])   \n",
    "# imagery_pred_latents_standardized = StandardScaler().fit_transform(results['imagery_predictions'])\n",
    "# plot_latents_tsne(train_latents, results['latents'], pred_latents_standardized, imagery_pred_latents_standardized, \"standardized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE for Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_betas_tsne(train_betas, test_betas, title, train_subset=None):\n",
    "#     if train_subset is not None:\n",
    "#         train_betas_subset = train_betas[np.random.choice(range(len(train_betas)), size=train_subset, replace=False)]\n",
    "#     else:\n",
    "#         train_betas_subset = train_betas\n",
    "#     train_test = np.concatenate((train_betas_subset, test_betas))\n",
    "#     tsne = TSNE(n_components=2, learning_rate='auto', verbose=1, n_jobs=10, n_iter=1000)\n",
    "#     X_embedded = tsne.fit_transform(train_test)\n",
    "    \n",
    "#     print(X_embedded.shape)\n",
    "#     assert X_embedded.shape[1] == 2\n",
    "#     hue = ['train'] * len(train_betas_subset) + ['test'] * len(test_betas)\n",
    "#     # alpha = [1] * len(test_betas) + [0.3] * len(train_betas_subset)\n",
    "    \n",
    "#     plt.figure(figsize=(20, 12))\n",
    "#     sns.scatterplot(\n",
    "#         x = X_embedded[:, 0], y = X_embedded[:, 1],\n",
    "#         hue = hue,\n",
    "#         # alpha = alpha\n",
    "#     )\n",
    "#     plt.title(title)\n",
    "\n",
    "# plot_betas_tsne(train_fmri_betas, test_fmri_betas, title=\"not standardized\")\n",
    "# plot_betas_tsne(train_fmri_betas_standardized, test_fmri_betas_standardized, \"standardized\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "64c2261fd1335a391d209058834a77a3cc43bbc1dadc63860e2129a303b1f182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
