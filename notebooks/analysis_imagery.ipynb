{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from PIL import Image, ImageColor\n",
    "import matplotlib.colors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import LATENT_FEATURES_DIR, RESULTS_DIR, FMRI_BETAS_SURFACE_DIR, STIM_INFO_PATH, COCO_IMAGES_DIR, METRIC_CROSS_DECODING, FMRI_DATA_DIR, DECODER_ADDITIONAL_TEST_OUT_DIR, FMRI_BIDS_DATA_DIR, SUBJECTS_ADDITIONAL_TEST\n",
    "from analyses.decoding.ridge_regression_decoding import NUM_CV_SPLITS, pairwise_accuracy, get_run_str, RESULTS_FILE, PREDICTIONS_FILE\n",
    "from data import MODALITY_AGNOSTIC, MODALITY_SPECIFIC_IMAGES, MODALITY_SPECIFIC_CAPTIONS, TRAINING_MODES, CAPTION, IMAGE, TEST_SPLITS, LatentFeatsConfig, get_stim_info, SPLIT_TRAIN, TEST_IMAGES, TEST_CAPTIONS, SPLIT_IMAGERY, SPLIT_IMAGERY_WEAK, get_latents_for_splits, standardize_latents, IMAGERY\n",
    "from eval import ACC_MODALITY_AGNOSTIC, ACC_CAPTIONS, ACC_IMAGES, ACC_CROSS_IMAGES_TO_CAPTIONS, ACC_CROSS_CAPTIONS_TO_IMAGES, ACC_IMAGERY, ACC_IMAGERY_WHOLE_TEST, get_distance_matrix\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from notebook_utils import load_predictions, load_betas, get_data_default_feats\n",
    "from glob import glob\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_data():\n",
    "    data = []\n",
    "\n",
    "    result_files = sorted(glob(f\"{DECODER_ADDITIONAL_TEST_OUT_DIR}/*/*/*/results.csv\"))\n",
    "    for result_file_path in tqdm(result_files):\n",
    "        results = pd.read_csv(result_file_path)\n",
    "        data.append(results)\n",
    "\n",
    "    data = pd.concat(data, ignore_index=True)\n",
    "    data[\"mask\"] = data[\"mask\"].fillna(\"whole_brain\")\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_results_data()\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "display(data)\n",
    "\n",
    "print(f\"Subjects: {data.subject.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = get_data_default_feats(data).copy()\n",
    "\n",
    "# LATENT_MODE = 'all_candidate_latents'\n",
    "LATENT_MODE = 'limited_candidate_latents'\n",
    "MASK = 'whole_brain'\n",
    "TRAINING_SPLITS = 'train'\n",
    "MODEL = 'imagebind'\n",
    "\n",
    "filtered = filtered[filtered.model == MODEL]\n",
    "filtered = filtered[filtered.standardized_predictions == 'True']\n",
    "filtered = filtered[filtered.training_splits == TRAINING_SPLITS]\n",
    "filtered = filtered[filtered.latents == LATENT_MODE]\n",
    "filtered = filtered[filtered['mask'] == MASK]\n",
    "filtered = filtered[filtered.imagery_samples_weight.isna()]\n",
    "filtered = filtered[filtered.surface == True]\n",
    "\n",
    "# print(filtered.groupby(['metric', 'training_mode']).agg(num_subjects=('value', 'size')).reset_index())\n",
    "NUM_SUBJECTS = len(SUBJECTS_ADDITIONAL_TEST)\n",
    "expected_len = NUM_SUBJECTS * len(filtered.metric.unique()) * len(filtered.training_mode.unique())\n",
    "assert len(filtered) == expected_len, filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagery vs imagery (weak)\n",
    "Imagery (weak) is worse because of larger candidate set. If we use comparable candidate sets the performance for imagery (weak) decoding is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = filtered.copy()\n",
    "\n",
    "ORDER = ['captions', 'images', 'agnostic']\n",
    "HUE_ORDER = ['imagery', 'imagery_weak']\n",
    "PALETTE = ['red', 'salmon']\n",
    "# sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.title('imagery decoding', y=0.95, fontsize=20)\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"training_mode\", y=\"value\", hue=\"metric\", order=ORDER, hue_order=HUE_ORDER, palette=PALETTE)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.5, 1))\n",
    "sns.despine()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"imagery_vs_imagery_weak.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance advantage of mod-agnostic decoders over mod-specific for imagery decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_transform = {\n",
    "    'sub-01': 'Subject 1',\n",
    "    'sub-02': 'Subject 2',\n",
    "    'sub-03': 'Subject 3',\n",
    "    'sub-04': 'Subject 4',\n",
    "    'sub-05': 'Subject 5',\n",
    "    'sub-07': 'Subject 6',\n",
    "}\n",
    "def subjects_for_plotting(data):\n",
    "    return data['subject'].apply(lambda subj: subj_transform[subj])\n",
    "\n",
    "training_mode_transform = {\n",
    "    MODALITY_AGNOSTIC: 'Modality-agnostic decoder',\n",
    "    MODALITY_SPECIFIC_IMAGES: 'Modality-specific (images)',\n",
    "    MODALITY_SPECIFIC_CAPTIONS: 'Modality-specific (captions)',\n",
    "}\n",
    "def decoder_for_plotting(data):\n",
    "    return data['training_mode'].apply(lambda tm: training_mode_transform[tm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = filtered.copy()\n",
    "\n",
    "DECODER_TYPES = ['Modality-agnostic decoder', 'Modality-specific (images)', 'Modality-specific (captions)']\n",
    "METRICS = ['imagery_weak']\n",
    "plt.figure(figsize=(11,5))\n",
    "to_plot['subject'] = subjects_for_plotting(to_plot)\n",
    "to_plot['decoder_type'] = decoder_for_plotting(to_plot)\n",
    "\n",
    "to_plot = to_plot[to_plot.metric.isin(METRICS)]\n",
    "to_plot = to_plot[to_plot.decoder_type.isin(DECODER_TYPES)]\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"decoder_type\", y=\"value\", order=DECODER_TYPES, errorbar=None, color='gray')\n",
    "ax = sns.pointplot(data=to_plot, x=\"decoder_type\", y=\"value\", order=DECODER_TYPES, hue=\"subject\")\n",
    "lgd = ax.legend(loc='upper left', ncols=5, title='', bbox_to_anchor=(0,1.05), frameon=False)\n",
    "# lgd.get_frame().set_linewidth(0.0)\n",
    "\n",
    "plt.ylabel('Pairwise Accuracy')\n",
    "plt.xlabel('')\n",
    "plt.ylim((0.5, 1))\n",
    "sns.despine()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"imagery_weak_decoding_decoder_comparison.png\"), bbox_inches='tight', pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(to_plot)\n",
    "acc_images = to_plot[to_plot.training_mode == 'images'].value\n",
    "acc_captions = to_plot[to_plot.training_mode == 'captions'].value\n",
    "acc_agnostic = to_plot[to_plot.training_mode == 'agnostic'].value\n",
    "assert len(acc_images) == len(SUBJECTS_ADDITIONAL_TEST)\n",
    "assert len(acc_captions) == len(SUBJECTS_ADDITIONAL_TEST)\n",
    "assert len(acc_agnostic) == len(SUBJECTS_ADDITIONAL_TEST)\n",
    "# print(acc_images.mean())\n",
    "# print(acc_agnostic.mean())\n",
    "print('ttest (agnostic vs. images): ', ttest_rel(acc_agnostic, acc_images, alternative='greater'))\n",
    "print('ttest (agnostic vs. captions): ', ttest_rel(acc_agnostic, acc_captions, alternative='greater'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_plot = filtered.copy()\n",
    "\n",
    "# TRAINING_MODES = ['agnostic']\n",
    "# METRICS = ['imagery_weak', 'test_caption_attended']\n",
    "# sns.set(font_scale=1.3)\n",
    "# plt.figure(figsize=(17,10))\n",
    "# plt.title('decoding', y=0.95, fontsize=20)\n",
    "\n",
    "# to_plot = to_plot[to_plot.metric.isin(METRICS)]\n",
    "# to_plot = to_plot[to_plot.training_mode.isin(TRAINING_MODES)]\n",
    "\n",
    "# ax = sns.barplot(data=to_plot, x=\"metric\", y=\"value\", errorbar=None, color='gray')\n",
    "# # ax = sns.scatterplot(data=to_plot, x=\"subject\", y=\"value\", hue=\"training_mode\")\n",
    "# ax = sns.pointplot(data=to_plot, x=\"metric\", y=\"value\", hue=\"subject\")\n",
    "\n",
    "# plt.ylabel('pairwise accuracy')\n",
    "# plt.ylim((0.5, 1.1))\n",
    "# plt.savefig(os.path.join(RESULTS_DIR, f\"attention_modulation_imagery.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "\n",
    "# display(to_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For some participants, mod-specific decoders trained on captions do not generalize well to imagery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = filtered.copy()\n",
    "\n",
    "TRAINING_MODES = ['images', 'captions']\n",
    "METRICS = ['imagery_weak']\n",
    "# sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.title('imagery (weak) decoding', y=0.95, fontsize=20)\n",
    "\n",
    "to_plot = to_plot[to_plot.metric.isin(METRICS)]\n",
    "to_plot = to_plot[to_plot.training_mode.isin(TRAINING_MODES)]\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"training_mode\", y=\"value\", errorbar=None, color='gray')\n",
    "# ax = sns.scatterplot(data=to_plot, x=\"subject\", y=\"value\", hue=\"training_mode\")\n",
    "ax = sns.pointplot(data=to_plot, x=\"training_mode\", y=\"value\", hue=\"subject\")\n",
    "\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.5, 1))\n",
    "sns.despine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagery (weak) decoding with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = get_data_default_feats(data).copy()\n",
    "\n",
    "# LATENT_MODE = 'all_candidate_latents'\n",
    "LATENT_MODE = 'limited_candidate_latents'\n",
    "TRAINING_SPLITS = 'train'\n",
    "SUBJECTS = SUBJECTS_ADDITIONAL_TEST\n",
    "MODEL = 'imagebind'\n",
    "\n",
    "to_plot = to_plot[to_plot.model == MODEL]\n",
    "to_plot = to_plot[to_plot.standardized_predictions == 'True']\n",
    "to_plot = to_plot[to_plot.latents == LATENT_MODE]\n",
    "to_plot = to_plot[to_plot.metric == 'imagery_weak']\n",
    "to_plot = to_plot[to_plot.surface == True]\n",
    "to_plot = to_plot[to_plot.training_splits == TRAINING_SPLITS]\n",
    "to_plot = to_plot[to_plot.training_mode == 'agnostic']\n",
    "to_plot = to_plot[to_plot.subject.isin(SUBJECTS)]\n",
    "\n",
    "to_plot['mask'] = to_plot['mask'].apply(lambda x: os.path.basename(x))\n",
    "print(to_plot['mask'].unique())\n",
    "# MASKS = ['mod_agnostic_and_cross_threshold_0.01.p', 'mod_agnostic_and_cross_threshold_0.0001.p', 'whole_brain']\n",
    "# to_plot = to_plot[to_plot['mask'].isin(MASKS)]\n",
    "\n",
    "\n",
    "assert len(to_plot) == len(SUBJECTS) * len(to_plot['mask'].unique()) \n",
    "\n",
    "ORDER = None\n",
    "HUE_ORDER = [ \n",
    "    'random_1000',  'captions$test_caption_attended_1000_vertices.p', 'images$test_image_attended_1000_vertices.p',               'mod_invariant_increase_1000_vertices.p', 'mod_invariant_attended_1000_vertices.p',\n",
    "'random_10000', 'captions$test_caption_attended_10000_vertices.p',  'images$test_image_attended_10000_vertices.p',               'mod_invariant_increase_10000_vertices.p', 'mod_invariant_attended_10000_vertices.p',\n",
    "    'random_100000', 'captions$test_caption_attended_100000_vertices.p',  'images$test_image_attended_100000_vertices.p',               'mod_invariant_increase_100000_vertices.p', 'mod_invariant_attended_100000_vertices.p',\n",
    " # 'mod_agnostic_and_cross_lh_threshold_0.0001_cluster_0.p'\n",
    " # 'mod_agnostic_and_cross_rh_threshold_0.0001_cluster_0.p'\n",
    " # 'mod_agnostic_and_cross_rh_threshold_0.0001_cluster_1.p'\n",
    " # 'mod_agnostic_and_cross_threshold_0.0001.p'\n",
    " # 'mod_agnostic_and_cross_threshold_0.01.p'\n",
    " # 'mod_agnostic_and_cross_threshold_200000.0.p'\n",
    "    'whole_brain']\n",
    "\n",
    "PALETTE = 'tab10'\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('imagery (weak) decoding', y=0.95, fontsize=20)\n",
    "\n",
    "# ax = sns.barplot(data=to_plot, x='subject', y=\"value\", hue=\"mask\", order=ORDER, hue_order=HUE_ORDER, palette=PALETTE)\n",
    "ax = sns.barplot(data=to_plot, x=None, y=\"value\", hue=\"mask\", order=ORDER, hue_order=HUE_ORDER, palette=PALETTE)\n",
    "\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"imagery_weak_decoding_based_on_modality_agnostic_regions.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "\n",
    "\n",
    "display(to_plot.groupby(['mask']).agg(value=('value', 'mean'),n_vertices=('num_voxels', 'mean')).reset_index())\n",
    "\n",
    "\n",
    "acc_mask = to_plot[to_plot['mask']== 'mod_agnostic_and_cross_threshold_0.01.p'].value\n",
    "acc_whole_brain = to_plot[to_plot['mask'] == 'whole_brain'].value\n",
    "# print(acc_mask.mean())\n",
    "# print(acc_whole_brain.mean())\n",
    "# ttest_rel(acc_mask, acc_whole_brain, alternative='greater')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(predictions, originals, metric='cosine'):\n",
    "    dist = cdist(predictions, originals, metric=metric)\n",
    "    return dist\n",
    "    \n",
    "def dist_mat_to_pairwise_acc(dist_mat, stim_ids, print_details=False, reduce=True):\n",
    "    if reduce:\n",
    "        diag = dist_mat.diagonal().reshape(-1, 1)\n",
    "        comp_mat = diag < dist_mat\n",
    "        corrects = comp_mat.sum()\n",
    "        if print_details:\n",
    "            for i, stim_id in enumerate(stim_ids):\n",
    "                print(stim_id, end=': ')\n",
    "                print(f'{comp_mat[i].sum() / (len(comp_mat[i]) - 1):.2f}')\n",
    "        # subtract the number of elements of the diagonal as these values are always \"False\" (not smaller than themselves)\n",
    "        score = corrects / (dist_mat.size - diag.size)\n",
    "        return score\n",
    "    else:\n",
    "        diag = dist_mat.diagonal().reshape(-1, 1)\n",
    "        comp_mat = diag < dist_mat \n",
    "        # print(diag.shape)\n",
    "        # print(diag)\n",
    "        acc_per_pred = comp_mat.sum(axis=1) / (len(dist_mat) - 1)\n",
    "\n",
    "        return acc_per_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURFACE = True\n",
    "\n",
    "MODEL = \"imagebind\"\n",
    "\n",
    "SUBJECTS = SUBJECTS_ADDITIONAL_TEST\n",
    "\n",
    "TRAINING_MODES = [\"images\", \"agnostic\"]\n",
    "\n",
    "BETAS_SUFFIX = 'betas'\n",
    "BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "RESTANDARDIZE_PREDS = [SPLIT_IMAGERY_WEAK]\n",
    "\n",
    "FEATS = 'default'\n",
    "TEST_FEATS = 'default'\n",
    "VISION_FEATS = 'default'\n",
    "LANG_FEATS = 'default'\n",
    "FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "all_pairwise_accs = []\n",
    "for subj in SUBJECTS:\n",
    "    print(subj)\n",
    "    for training_mode in TRAINING_MODES:\n",
    "    \n",
    "        stim_ids_imagery, _ =  get_stim_info(subj, SPLIT_IMAGERY_WEAK)\n",
    "    \n",
    "        latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN, TEST_IMAGES, SPLIT_IMAGERY_WEAK], training_mode)\n",
    "        latents = standardize_latents(latents)\n",
    "    \n",
    "        predictions = load_predictions(BETAS_DIR, subj, training_mode, FEATS_CONFIG, surface=SURFACE)\n",
    "    \n",
    "        pred_latents_imagery = predictions[SPLIT_IMAGERY_WEAK]\n",
    "        if len(RESTANDARDIZE_PREDS)>0:\n",
    "            print(f'standardizing imagery predictions with {RESTANDARDIZE_PREDS}')\n",
    "            refs = np.concatenate([predictions[split] for split in RESTANDARDIZE_PREDS])\n",
    "            transform = StandardScaler().fit(refs)\n",
    "            pred_latents_imagery = transform.transform(pred_latents_imagery)\n",
    "\n",
    "        candidate_latents = latents[SPLIT_IMAGERY_WEAK]\n",
    "        candidate_latent_ids = stim_ids_imagery\n",
    "        \n",
    "        dist_mat = get_distance_matrix(pred_latents_imagery, candidate_latents)\n",
    "        scores = dist_mat_to_pairwise_acc(dist_mat, candidate_latent_ids, reduce=False)\n",
    "        # print(scores.mean())\n",
    "        for stim_id, score in zip(stim_ids_imagery, scores):\n",
    "            all_pairwise_accs.append({'value': score, 'training_mode': training_mode, 'subject': subj, 'stim_id': stim_id})\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(all_pairwise_accs)\n",
    "df['subj_stim_id'] = df['subject'] + '_' + df['stim_id'].astype(\"string\")\n",
    "# display(df)\n",
    "values_agnostic = df[df.training_mode == 'agnostic'].sort_values(['subject', 'stim_id'])\n",
    "values_images = df[df.training_mode == 'images'].sort_values(['subject', 'stim_id'])\n",
    "display(values_agnostic)\n",
    "display(values_images)\n",
    "\n",
    "# plt.figure(figsize=(40, 40))\n",
    "# ax = sns.pointplot(data=df, x=\"training_mode\", y=\"value\", hue=\"subj_stim_id\")\n",
    "\n",
    "ttest_rel(values_images.value, values_agnostic.value, alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the target features for Imagery (weak) decoding\n",
    "Imagebind avg features (average of vision and lang) seem to work best. Using only lang features is almost as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = data.copy()\n",
    "\n",
    "# LATENT_MODE = 'all_candidate_latents'\n",
    "LATENT_MODE = 'limited_candidate_latents'\n",
    "MASK = 'whole_brain'\n",
    "TRAINING_SPLITS = 'train'\n",
    "SUBJECTS = SUBJECTS_ADDITIONAL_TEST\n",
    "\n",
    "to_plot = to_plot[to_plot.model == 'imagebind']\n",
    "to_plot = to_plot[(to_plot.test_features == 'avg') | (to_plot.features != 'avg')]\n",
    "to_plot = to_plot[to_plot.standardized_predictions == 'True']\n",
    "to_plot = to_plot[to_plot.latents == LATENT_MODE]\n",
    "to_plot = to_plot[to_plot.metric == 'imagery_weak']\n",
    "to_plot = to_plot[to_plot.surface == True]\n",
    "to_plot = to_plot[to_plot.training_splits == TRAINING_SPLITS]\n",
    "to_plot = to_plot[to_plot.training_mode == 'agnostic']\n",
    "to_plot = to_plot[to_plot['mask'] == MASK]\n",
    "to_plot = to_plot[to_plot.subject.isin(SUBJECTS)]\n",
    "\n",
    "\n",
    "FEAT_OPTIONS = ['avg', 'lang', 'vision']\n",
    "display(to_plot)\n",
    "\n",
    "assert len(to_plot) == len(SUBJECTS) * len(to_plot.features.unique())\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('imagery (weak) decoding', y=0.95, fontsize=20)\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"features\", y=\"value\", order=FEAT_OPTIONS)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"imagery_weak_feat_comparison.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the target features for Imagery (weak) decoding at test time\n",
    "Imagebind avg and lang features work equally well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = data.copy()\n",
    "\n",
    "# LATENT_MODE = 'all_candidate_latents'\n",
    "LATENT_MODE = 'limited_candidate_latents'\n",
    "MASK = 'whole_brain'\n",
    "TRAINING_SPLITS = 'train'\n",
    "SUBJECTS = SUBJECTS_ADDITIONAL_TEST\n",
    "\n",
    "to_plot = to_plot[to_plot.model == 'imagebind']\n",
    "to_plot = to_plot[to_plot.features == 'avg']\n",
    "to_plot = to_plot[to_plot.standardized_predictions == 'True']\n",
    "to_plot = to_plot[to_plot.latents == LATENT_MODE]\n",
    "to_plot = to_plot[to_plot.metric == 'imagery_weak']\n",
    "to_plot = to_plot[to_plot.surface == True]\n",
    "to_plot = to_plot[to_plot.training_splits == TRAINING_SPLITS]\n",
    "to_plot = to_plot[to_plot.training_mode == 'agnostic']\n",
    "to_plot = to_plot[to_plot['mask'] == MASK]\n",
    "to_plot = to_plot[to_plot.subject.isin(SUBJECTS)]\n",
    "\n",
    "\n",
    "FEAT_OPTIONS = ['avg', 'lang']\n",
    "display(to_plot)\n",
    "\n",
    "assert len(to_plot) == len(SUBJECTS) * len(to_plot.test_features.unique())\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('imagery (weak) decoding', y=0.95, fontsize=20)\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"test_features\", y=\"value\", order=FEAT_OPTIONS)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"imagery_weak_test_feat_comparison.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = get_data_default_feats(data).copy()\n",
    "\n",
    "# LATENT_MODE = 'all_candidate_latents'\n",
    "LATENT_MODE = 'limited_candidate_latents'\n",
    "MASK = 'whole_brain'\n",
    "TRAINING_SPLITS = 'train'\n",
    "SUBJECTS = SUBJECTS_ADDITIONAL_TEST\n",
    "\n",
    "to_plot = to_plot[to_plot.standardized_predictions == 'True']\n",
    "to_plot = to_plot[to_plot.latents == LATENT_MODE]\n",
    "to_plot = to_plot[to_plot.metric == 'imagery_weak']\n",
    "to_plot = to_plot[to_plot.surface == True]\n",
    "to_plot = to_plot[to_plot.training_splits == TRAINING_SPLITS]\n",
    "to_plot = to_plot[to_plot.training_mode == 'agnostic']\n",
    "to_plot = to_plot[to_plot['mask'] == MASK]\n",
    "to_plot = to_plot[to_plot.subject.isin(SUBJECTS)]\n",
    "\n",
    "display(to_plot)\n",
    "\n",
    "assert len(to_plot) == len(SUBJECTS) * len(to_plot.model.unique())\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"model\", y=\"value\", order=None)\n",
    "plt.title('imagery (weak) decoding', y=0.95, fontsize=20)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n",
    "# plt.savefig(os.path.join(RESULTS_DIR, f\"imager_weak_feat_comparison.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagery decoding with varying standardization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = get_data_default_feats(data).copy()\n",
    "\n",
    "LATENT_MODE = 'all_candidate_latents'\n",
    "# LATENT_MODE = 'limited_candidate_latents'\n",
    "MASK = 'whole_brain'\n",
    "TRAINING_SPLITS = 'train'\n",
    "MODEL = 'imagebind'\n",
    "\n",
    "to_plot = to_plot[to_plot.model == MODEL]\n",
    "# to_plot = to_plot[to_plot.standardized_predictions == 'True']\n",
    "to_plot = to_plot[to_plot.latents == LATENT_MODE]\n",
    "to_plot = to_plot[to_plot.metric == 'imagery']\n",
    "to_plot = to_plot[to_plot.surface == True]\n",
    "to_plot = to_plot[to_plot.training_splits == TRAINING_SPLITS]\n",
    "to_plot = to_plot[to_plot['mask'] == MASK]\n",
    "\n",
    "assert len(to_plot) == NUM_SUBJECTS * len(to_plot.standardized_predictions.unique()) * len(to_plot.training_mode.unique())\n",
    "\n",
    "ORDER = ['captions', 'images', 'agnostic']\n",
    "HUE_ORDER = None\n",
    "\n",
    "PALETTE = 'tab10'\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('imagery decoding', y=0.95, fontsize=20)\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"training_mode\", y=\"value\", hue=\"standardized_predictions\", order=ORDER, hue_order=HUE_ORDER, palette=PALETTE)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n",
    "# plt.savefig(os.path.join(RESULTS_DIR, f\"vary_standardization.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagery decoding with varying training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = get_data_default_feats(data).copy()\n",
    "\n",
    "\n",
    "LATENT_MODE = 'all_candidate_latents'\n",
    "# LATENT_MODE = 'limited_candidate_latents'\n",
    "MASK = 'whole_brain'\n",
    "TRAINING_SPLITS = 'train'\n",
    "\n",
    "print(to_plot.training_splits.unique())\n",
    "MODEL = 'imagebind'\n",
    "to_plot = to_plot[to_plot.model == MODEL]\n",
    "# to_plot = to_plot[to_plot.standardized_predictions == 'True']\n",
    "to_plot = to_plot[to_plot.latents == LATENT_MODE]\n",
    "to_plot = to_plot[to_plot.metric == 'imagery']\n",
    "# to_plot = to_plot[to_plot.training_splits == TRAINING_SPLITS]\n",
    "to_plot = to_plot[to_plot['mask'] == MASK]\n",
    "to_plot = to_plot[to_plot.surface == True]\n",
    "to_plot = to_plot[to_plot.training_mode == 'agnostic']\n",
    "to_plot = to_plot[to_plot.imagery_samples_weight.isna()]\n",
    "\n",
    "\n",
    "# display(to_plot)\n",
    "# assert len(to_plot) == NUM_SUBJECTS * len(to_plot.standardized_predictions.unique()) * len(to_plot.training_splits.unique()), to_plot\n",
    "\n",
    "PALETTE = 'tab10'#['red', 'salmon']\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('imagery decoding', y=0.95, fontsize=20)\n",
    "\n",
    "ax = sns.barplot(data=to_plot, x=\"training_splits\", y=\"value\", hue=\"standardized_predictions\", palette=PALETTE)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"attention_modulation_imagery.png\"), bbox_inches='tight', pad_inches=0, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagery decoding with varying training sets and sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = data.copy()\n",
    "\n",
    "\n",
    "LATENT_MODE = 'all_candidate_latents'\n",
    "# LATENT_MODE = 'limited_candidate_latents'\n",
    "MASK = 'whole_brain'\n",
    "TRAINING_SPLITS = 'train_imagery_weak'\n",
    "\n",
    "print(to_plot.training_splits.unique())\n",
    "\n",
    "to_plot = to_plot[to_plot.latents == LATENT_MODE]\n",
    "to_plot = to_plot[to_plot.metric == 'imagery']\n",
    "to_plot = to_plot[to_plot['mask'] == MASK]\n",
    "to_plot = to_plot[to_plot.surface == True]\n",
    "to_plot = to_plot[to_plot.training_mode == 'agnostic']\n",
    "to_plot = to_plot[to_plot.standardized_predictions == 'all_imagery']\n",
    "\n",
    "# print(to_plot[to_plot.training_splits == 'train'])\n",
    "\n",
    "to_plot = to_plot[to_plot.training_splits == TRAINING_SPLITS]\n",
    "\n",
    "\n",
    "to_plot.imagery_samples_weight.fillna(1.0, inplace=True)\n",
    "\n",
    "sample_weights = [1, 10, 100, 200, 500, 1000]\n",
    "to_plot = to_plot[to_plot.imagery_samples_weight.isin(sample_weights)]\n",
    "\n",
    "print(to_plot.imagery_samples_weight.unique())\n",
    "# display(to_plot)\n",
    "assert len(to_plot) == NUM_SUBJECTS * len(to_plot.standardized_predictions.unique()) * len(to_plot.imagery_samples_weight.unique()), len(to_plot)\n",
    "\n",
    "HUE_ORDER = np.sort(to_plot.imagery_samples_weight.unique())\n",
    "\n",
    "PALETTE = 'tab10'#['red', 'salmon']\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('imagery decoding', y=0.95, fontsize=20)\n",
    "ax = sns.barplot(data=to_plot, y=\"value\", hue=\"imagery_samples_weight\", hue_order=HUE_ORDER, palette=PALETTE)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n",
    "plt.axhline(y=0.78)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('imagery decoding', y=0.95, fontsize=20)\n",
    "ax = sns.barplot(data=to_plot, x=\"subject\", y=\"value\", hue=\"imagery_samples_weight\", hue_order=HUE_ORDER, palette=PALETTE)\n",
    "plt.ylabel('pairwise accuracy')\n",
    "plt.ylim((0.3, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction.feat_extraction_utils import CoCoDataset\n",
    "from pdf2image import convert_from_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_ds = CoCoDataset(COCO_IMAGES_DIR, STIM_INFO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(image, length=100):\n",
    "    if image.size[0] < image.size[1]:\n",
    "        resized_image = image.resize((length, int(image.size[1] * (length / image.size[0]))))\n",
    "        required_loss = (resized_image.size[1] - length)\n",
    "        resized_image = resized_image.crop(box=(0, required_loss / 2, length, resized_image.size[1] - required_loss / 2))\n",
    "    else:\n",
    "        resized_image = image.resize((int(image.size[0] * (length / image.size[1])), length))\n",
    "        required_loss = resized_image.size[0] - length\n",
    "        resized_image = resized_image.crop(box=(required_loss / 2, 0, resized_image.size[0] - required_loss / 2, length))\n",
    "    return resized_image\n",
    "\n",
    "def display_stimuli(coco_ids, imgs=True, caps=True):\n",
    "    if caps:\n",
    "        for coco_id in coco_ids:\n",
    "            print(coco_ds.captions[coco_id], end=\"\\n\")\n",
    "\n",
    "    if imgs:\n",
    "        imgs = [np.array(resize_img(coco_ds.get_img_by_coco_id(img_id))) for img_id in coco_ids]        \n",
    "        img = Image.fromarray(np.hstack(imgs))\n",
    "        display(img)\n",
    "\n",
    "def get_distance_matrix(predictions, originals, metric='cosine'):\n",
    "    dist = cdist(predictions, originals, metric=metric)\n",
    "    return dist\n",
    "    \n",
    "def dist_mat_to_pairwise_acc(dist_mat, stim_ids, print_details=False):\n",
    "    diag = dist_mat.diagonal().reshape(-1, 1)\n",
    "    comp_mat = diag < dist_mat\n",
    "    corrects = comp_mat.sum()\n",
    "    if print_details:\n",
    "        for i, stim_id in enumerate(stim_ids):\n",
    "            print(stim_id, end=': ')\n",
    "            print(f'{comp_mat[i].sum() / (len(comp_mat[i]) - 1):.2f}')\n",
    "    # subtract the number of elements of the diagonal as these values are always \"False\" (not smaller than themselves)\n",
    "    score = corrects / (dist_mat.size - diag.size)\n",
    "    return score\n",
    "\n",
    "def dist_mat_to_rankings(dist_mat, stim_ids, candidate_set_latent_ids):\n",
    "    all_ranks = []\n",
    "    for test_stimulus_id, nneighbors_row in zip(stim_ids, dist_mat):\n",
    "        nneighbors_ids = np.array(candidate_set_latent_ids)[np.argsort(nneighbors_row)]\n",
    "        rank = np.argwhere(nneighbors_ids == test_stimulus_id)[0][0] + 1\n",
    "\n",
    "        all_ranks.append(rank)\n",
    "            \n",
    "    return np.mean(all_ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors of imagery images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTSIZE = 13\n",
    "\n",
    "\n",
    "def plot_nn_table(stim_ids, nneighbors, subject, stim_type, out_file_name=None, img_length=150, hspace=0.2, wspace=0.05, similarities=None, similarties_pred_to_top_ranks=None):\n",
    "    if stim_type == IMAGERY:\n",
    "        stimulus_key = 'Imagery sketch and initial\\ninstruction'\n",
    "    elif stim_type == SPLIT_IMAGERY_WEAK:\n",
    "        stimulus_key = 'Ground Truth'\n",
    "    else:\n",
    "        stimulus_key = 'Stimulus'\n",
    "\n",
    "    figsize=(15,15)\n",
    "    \n",
    "    df = pd.DataFrame({stimulus_key: stim_ids} | {f'rank {i}': [n[i] for n in nneighbors] for i in range(len(nneighbors[0]))})\n",
    "\n",
    "    n_columns = len(nneighbors[0])+1\n",
    "    fig, axes = plt.subplots(len(stim_ids),n_columns, figsize=figsize) #, layout=\"constrained\"\n",
    "\n",
    "    if stim_type == IMAGERY:\n",
    "        fig.subplots_adjust(wspace=wspace, hspace=hspace, top=0.97, bottom=0.06, left=0.01, right=0.99)   \n",
    "    else:\n",
    "        fig.subplots_adjust(wspace=wspace, hspace=hspace, top=0.98, bottom=0.03, left=0.01, right=0.99)  \n",
    "       \n",
    "\n",
    "    for idx, (stim_id, neighbors, similarity, sim_ranks) in enumerate(zip(stim_ids, nneighbors, similarities, similarties_pred_to_top_ranks)):\n",
    "        caption = coco_ds.captions[stim_id].lower()\n",
    "        img = resize_img(coco_ds.get_img_by_coco_id(stim_id), length=img_length)\n",
    "\n",
    "        if stim_type == IMAGE:\n",
    "            axes[idx][0].imshow(img)\n",
    "        elif stim_type == SPLIT_IMAGERY_WEAK:\n",
    "            axes[idx][0].imshow(img)\n",
    "            txt = axes[idx][0].text(0, 155, caption, ha='left', wrap=True, fontsize=FONTSIZE, verticalalignment='top', \n",
    "                                   bbox=dict(boxstyle='square,pad=0', facecolor='none', edgecolor='none'))\n",
    "            txt._get_wrap_line_width = lambda : img_length*4\n",
    "        elif stim_type in [CAPTION]:\n",
    "            img = Image.fromarray(np.full((img_length, img_length, 3), 255, dtype=np.uint8), \"RGB\")\n",
    "            axes[idx][0].imshow(img)\n",
    "            txt = axes[idx][0].text(0, img_length/2, caption+'\\n\\n'+f'(cosine distance: {np.round(distance, 2)})', ha='left', wrap=True, fontsize=FONTSIZE,# verticalalignment='top', \n",
    "                                   bbox=dict(boxstyle='square,pad=0', facecolor='none', edgecolor='none'))\n",
    "            txt._get_wrap_line_width = lambda : img_length*4\n",
    "        elif stim_type == IMAGERY:\n",
    "            drawing_path = os.path.join(FMRI_BIDS_DATA_DIR, \"stimuli\", \"imagery_drawings\", f\"{subject}_imagery_{idx+1}.pdf\")\n",
    "            img_drawing = convert_from_path(drawing_path)[0]\n",
    "            img_drawing = resize_img(img_drawing, length=img_length)\n",
    "            \n",
    "            axes[idx][0].imshow(img_drawing)\n",
    "            txt = axes[idx][0].text(0, 155, caption, ha='left', wrap=True, fontsize=FONTSIZE, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='square,pad=0', facecolor='none', edgecolor='none'))\n",
    "            txt._get_wrap_line_width = lambda : img_length*4\n",
    "            # axes[idx][1].axis('off')\n",
    "            \n",
    "        axes[idx][0].axis('off')\n",
    "\n",
    "        if stim_type == SPLIT_IMAGERY_WEAK:\n",
    "            axes[idx][0].set_title(f'{stimulus_key} (cos={similarity:.2f})', fontweight=\"bold\", fontsize=12.5)\n",
    "        else:\n",
    "            if idx == 0:\n",
    "                axes[idx][0].set_title(f'{stimulus_key}', fontweight=\"bold\")\n",
    "    \n",
    "\n",
    "        for n_id, (neighbor_id, sim) in enumerate(zip(neighbors, sim_ranks)):\n",
    "            caption = coco_ds.captions[neighbor_id].lower()\n",
    "            img = resize_img(coco_ds.get_img_by_coco_id(neighbor_id), length=img_length)\n",
    "            axes[idx][n_id+1].imshow(img)\n",
    "            axes[idx][n_id+1].axis('off')\n",
    "            if stim_type == SPLIT_IMAGERY_WEAK:\n",
    "               axes[idx][n_id+1].set_title(f'Rank {n_id} (cos={sim:.2f})', fontweight=\"bold\", fontsize=12.5)\n",
    "            else:\n",
    "                if idx == 0:\n",
    "                    axes[idx][n_id+1].set_title(f'Rank {n_id} (cos={sim:.2f})', fontweight=\"bold\")\n",
    "\n",
    "            txt = axes[idx][n_id+1].text(0, 155, caption, ha='left', wrap=True, fontsize=FONTSIZE, verticalalignment='top',\n",
    "                                   bbox=dict(boxstyle='square,pad=0', facecolor='none', edgecolor='none'))\n",
    "            if stim_type == IMAGERY:\n",
    "                txt._get_wrap_line_width = lambda : img_length*4\n",
    "            else:\n",
    "                txt._get_wrap_line_width = lambda : img_length*4\n",
    "\n",
    " \n",
    "    if out_file_name is not None:\n",
    "        out_path = os.path.join(RESULTS_DIR, \"analysis_ranking\", out_file_name)\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        plt.savefig(out_path, dpi=250)\n",
    "\n",
    "        plt.close(fig)\n",
    "        img = mpimg.imread(out_path)\n",
    "        plt.figure(figsize=(15,15))\n",
    "        # plt.figure(figsize=(3*len(nneighbors),3*len(nneighbors)))\n",
    "        imgplot = plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def analysis_ranking(test_preds, test_stim_ids, candidate_latents, candidate_latent_ids, subject, stim_type, n_samples=5, num_neighbors=5, out_file_name=None, hspace=0.2, wspace=0.05, subsample='random'):\n",
    "    sim_mat = cosine_similarity(test_preds, candidate_latents)\n",
    "    dist_mat = get_distance_matrix(test_preds, candidate_latents)\n",
    "\n",
    "    acc = dist_mat_to_pairwise_acc(dist_mat, test_stim_ids)\n",
    "    print(f'pairwise acc: {acc:.3f}')\n",
    "\n",
    "    distances = None\n",
    "    if stim_type != IMAGERY:        \n",
    "        distances_to_target = []\n",
    "        similarities_pred_to_target = []\n",
    "        similarties_pred_to_top_ranks = []\n",
    "        for test_stim_id, dist_row, sim_row in zip(test_stim_ids, dist_mat, sim_mat):\n",
    "            target_idx = np.where(candidate_latent_ids == test_stim_id)[0]\n",
    "            distance_to_target = dist_row[target_idx][0]\n",
    "            sim_to_target = sim_row[target_idx][0]\n",
    "            distances_to_target.append(distance_to_target)\n",
    "            similarities_pred_to_target.append(sim_to_target)\n",
    "            # print(sim_row)\n",
    "            sim_to_top_ranked = np.sort(sim_row)[-num_neighbors:][::-1]\n",
    "            \n",
    "            similarties_pred_to_top_ranks.append(sim_to_top_ranked)\n",
    "        distances_to_target = np.array(distances_to_target)\n",
    "        similarities_pred_to_target = np.array(similarities_pred_to_target)\n",
    "\n",
    "        if subsample == 'best':\n",
    "            best_indices = np.argsort(similarities_pred_to_target)[-n_samples:][::-1]\n",
    "            sampled_ids = best_indices\n",
    "        elif subsample == 'worst':\n",
    "            worst_indices = np.argsort(similarities_pred_to_target)[:n_samples][::-1]\n",
    "            sampled_ids = worst_indices\n",
    "        else:\n",
    "            np.random.seed(1)\n",
    "            sampled_ids = np.random.choice(range(len(test_stim_ids)), n_samples, replace=False)\n",
    "            \n",
    "        test_stim_ids = np.array(test_stim_ids)[sampled_ids]\n",
    "        dist_mat = dist_mat[sampled_ids]\n",
    "        distances = distances_to_target[sampled_ids]\n",
    "        similarties_pred_to_top_ranks = np.array(similarties_pred_to_top_ranks)[sampled_ids]\n",
    "        # print('Distances to target:', distances)\n",
    "\n",
    "        similarities = similarities_pred_to_target[sampled_ids]\n",
    "        print('Similarities to target:', similarities)\n",
    "        # print(similarties_pred_to_top_ranks)\n",
    "\n",
    "   \n",
    "    nneighbors = [np.array(candidate_latent_ids)[np.argsort(nneighbors_row)][:num_neighbors] for nneighbors_row in dist_mat]\n",
    "    \n",
    "    # print(nneighbors)    \n",
    "    plot_nn_table(test_stim_ids, nneighbors, subject, stim_type, out_file_name, hspace=hspace, wspace=wspace, similarities=similarities, similarties_pred_to_top_ranks=similarties_pred_to_top_ranks)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbors of imagery trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"imagebind\"\n",
    "TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "FEATS = 'default'\n",
    "TEST_FEATS = 'default'\n",
    "VISION_FEATS = 'default'\n",
    "LANG_FEATS = 'default'\n",
    "FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "all_train_stim_ids = []\n",
    "all_train_latents = []\n",
    "for subj in tqdm(SUBJECTS_ADDITIONAL_TEST):\n",
    "    stim_ids, _ = get_stim_info(subj, SPLIT_TRAIN)\n",
    "    \n",
    "    latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN], TRAINING_MODE)\n",
    "    latents = standardize_latents(latents)\n",
    "\n",
    "    all_train_stim_ids.append(stim_ids)\n",
    "    all_train_latents.append(latents[SPLIT_TRAIN])\n",
    "plt.axis('off')\n",
    "all_train_stim_ids = np.concatenate(all_train_stim_ids)\n",
    "all_train_latents = np.concatenate(all_train_latents)\n",
    "\n",
    "unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "unique_train_latents = all_train_latents[indices]\n",
    "print(len(unique_stim_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ioff()\n",
    "# N_NEIGHBORS = 5\n",
    "# N_SAMPLES = 3\n",
    "\n",
    "# WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET = True\n",
    "# SURFACE = True\n",
    "\n",
    "# MODEL = \"imagebind\"\n",
    "\n",
    "# TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "# MASK = None\n",
    "\n",
    "# BETAS_SUFFIX = 'betas'\n",
    "\n",
    "# BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "# RESTANDARDIZE_PREDS = [SPLIT_IMAGERY]\n",
    "# TRAINING_SPLITS = [SPLIT_TRAIN]\n",
    "# IMAGERY_SAMPLES_WEIGHT = None\n",
    "\n",
    "# FEATS = 'default'\n",
    "# TEST_FEATS = 'default'\n",
    "# VISION_FEATS = 'default'\n",
    "# LANG_FEATS = 'default'\n",
    "# FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "# all_pairwise_accs = []\n",
    "# for subj in SUBJECTS_ADDITIONAL_TEST:\n",
    "#     print(subj)\n",
    "\n",
    "#     stim_ids_test, _ = get_stim_info(subj, TEST_IMAGES)\n",
    "#     stim_ids_imagery, _ =  get_stim_info(subj, SPLIT_IMAGERY)\n",
    "\n",
    "#     latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN, TEST_IMAGES, SPLIT_IMAGERY], TRAINING_MODE)\n",
    "#     latents = standardize_latents(latents)\n",
    "\n",
    "#     predictions = load_predictions(BETAS_DIR, subj, TRAINING_MODE, FEATS_CONFIG, surface=SURFACE, mask=MASK, training_splits=TRAINING_SPLITS, imagery_samples_weight=IMAGERY_SAMPLES_WEIGHT)\n",
    "\n",
    "#     pred_latents_imagery = predictions[SPLIT_IMAGERY]\n",
    "#     if len(RESTANDARDIZE_PREDS)>0:\n",
    "#         print('standardizing imagery predictions')\n",
    "#         refs = np.concatenate([predictions[split] for split in RESTANDARDIZE_PREDS])\n",
    "#         transform = StandardScaler().fit(refs)\n",
    "#         pred_latents_imagery = transform.transform(pred_latents_imagery)\n",
    "#         # pred_latents_imagery = StandardScaler().fit_transform(pred_latents_imagery)\n",
    "\n",
    "#     # test_stim_ids_mod = results['stimulus_ids'][results['stimulus_types'] == IMAGE]\n",
    "#     # test_latents_mod = results['latents'][results['stimulus_types'] == IMAGE]\n",
    "\n",
    "#     if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "#         # # account for case that sometimes both the image and the caption are part of the training set\n",
    "#         # unique_stim_ids, indices = np.unique(stim_ids, return_index=True)\n",
    "#         # unique_train_latents = train_latents[indices]\n",
    "#         unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "#         unique_train_latents = all_train_latents[indices]\n",
    "#         print('candidate set size: ', len(unique_stim_ids))\n",
    "\n",
    "#         candidate_latents = np.concatenate((latents[SPLIT_IMAGERY], latents[TEST_IMAGES], unique_train_latents))\n",
    "#         candidate_latent_ids = np.concatenate((stim_ids_imagery, stim_ids_test, unique_stim_ids))\n",
    "#     else:\n",
    "#         candidate_latents = latents[SPLIT_IMAGERY]\n",
    "#         candidate_latent_ids = stim_ids_imagery\n",
    "    \n",
    "#     acc = analysis_ranking(pred_latents_imagery, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, IMAGERY, N_SAMPLES, N_NEIGHBORS, out_file_name=f\"{IMAGERY}_{TRAINING_MODE}_decoder_{subj}.png\", hspace=0.2)\n",
    "#     all_pairwise_accs.append(acc)\n",
    "\n",
    "\n",
    "# print(f'Mean pairwise acc: {np.mean(all_pairwise_accs):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.ioff()\n",
    "# N_NEIGHBORS = 5\n",
    "# N_SAMPLES = 3\n",
    "\n",
    "# WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET = True\n",
    "# SURFACE = True\n",
    "\n",
    "# MODEL = \"imagebind\"\n",
    "\n",
    "# TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "# MASK = None\n",
    "\n",
    "# BETAS_SUFFIX = 'betas'\n",
    "\n",
    "# BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "# RESTANDARDIZE_PREDS = [SPLIT_IMAGERY]\n",
    "# TRAINING_SPLITS = [SPLIT_TRAIN, SPLIT_IMAGERY_WEAK]\n",
    "# IMAGERY_SAMPLES_WEIGHT = 500\n",
    "\n",
    "# FEATS = 'default'\n",
    "# TEST_FEATS = 'default'\n",
    "# VISION_FEATS = 'default'\n",
    "# LANG_FEATS = 'default'\n",
    "# FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "# all_pairwise_accs = []\n",
    "# for subj in SUBJECTS_ADDITIONAL_TEST:\n",
    "#     print(subj)\n",
    "\n",
    "#     stim_ids_test, _ = get_stim_info(subj, TEST_IMAGES)\n",
    "#     stim_ids_imagery, _ =  get_stim_info(subj, SPLIT_IMAGERY)\n",
    "\n",
    "#     latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN, TEST_IMAGES, SPLIT_IMAGERY], TRAINING_MODE)\n",
    "#     latents = standardize_latents(latents)\n",
    "\n",
    "#     predictions = load_predictions(BETAS_DIR, subj, TRAINING_MODE, FEATS_CONFIG, surface=SURFACE, mask=MASK, training_splits=TRAINING_SPLITS, imagery_samples_weight=IMAGERY_SAMPLES_WEIGHT)\n",
    "\n",
    "#     pred_latents_imagery = predictions[SPLIT_IMAGERY]\n",
    "#     if len(RESTANDARDIZE_PREDS)>0:\n",
    "#         print('standardizing imagery predictions')\n",
    "#         refs = np.concatenate([predictions[split] for split in RESTANDARDIZE_PREDS])\n",
    "#         print(len(refs))\n",
    "#         transform = StandardScaler().fit(refs)\n",
    "#         pred_latents_imagery = transform.transform(pred_latents_imagery)\n",
    "#         # pred_latents_imagery = StandardScaler().fit_transform(pred_latents_imagery)\n",
    "\n",
    "#     # test_stim_ids_mod = results['stimulus_ids'][results['stimulus_types'] == IMAGE]\n",
    "#     # test_latents_mod = results['latents'][results['stimulus_types'] == IMAGE]\n",
    "\n",
    "#     if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "#         # # account for case that sometimes both the image and the caption are part of the training set\n",
    "#         # unique_stim_ids, indices = np.unique(stim_ids, return_index=True)\n",
    "#         # unique_train_latents = train_latents[indices]\n",
    "#         unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "#         unique_train_latents = all_train_latents[indices]\n",
    "#         print('candidate set size: ', len(unique_stim_ids))\n",
    "\n",
    "#         candidate_latents = np.concatenate((latents[SPLIT_IMAGERY], latents[TEST_IMAGES], unique_train_latents))\n",
    "#         candidate_latent_ids = np.concatenate((stim_ids_imagery, stim_ids_test, unique_stim_ids))\n",
    "#     else:\n",
    "#         candidate_latents = latents[SPLIT_IMAGERY]\n",
    "#         candidate_latent_ids = stim_ids_imagery\n",
    "\n",
    "#     out_file_name = f\"{IMAGERY}_{TRAINING_MODE}_decoder_{subj}_train_with_imagery_samples_weight_{IMAGERY_SAMPLES_WEIGHT}.png\"\n",
    "#     acc = analysis_ranking(pred_latents_imagery, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, IMAGERY, N_SAMPLES, N_NEIGHBORS, out_file_name=out_file_name, hspace=0.2)\n",
    "#     all_pairwise_accs.append(acc)\n",
    "\n",
    "# print(f'Mean pairwise acc: {np.mean(all_pairwise_accs):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TEST_IMAGES_ATTENDED, TEST_CAPTIONS_ATTENDED, TEST_IMAGES_UNATTENDED, TEST_CAPTIONS_UNATTENDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak imagery decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 5\n",
    "N_SAMPLES = 5\n",
    "\n",
    "WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET = False\n",
    "SURFACE = True\n",
    "MODEL = \"imagebind\"\n",
    "\n",
    "TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "MASK = None\n",
    "\n",
    "BETAS_SUFFIX = 'betas'\n",
    "\n",
    "BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "RESTANDARDIZE_PREDS = [SPLIT_IMAGERY_WEAK]\n",
    "\n",
    "FEATS = 'default'\n",
    "TEST_FEATS = 'default'\n",
    "VISION_FEATS = 'default'\n",
    "LANG_FEATS = 'default'\n",
    "FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "all_pairwise_accs = []\n",
    "all_preds = []\n",
    "\n",
    "if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "    all_train_stim_ids = []\n",
    "    all_train_latents = []\n",
    "    for subj in tqdm(SUBJECTS_ADDITIONAL_TEST):\n",
    "        stim_ids, _ = get_stim_info(subj, SPLIT_TRAIN)\n",
    "        \n",
    "        latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN], TRAINING_MODE)\n",
    "        latents = standardize_latents(latents)\n",
    "    \n",
    "        all_train_stim_ids.append(stim_ids)\n",
    "        all_train_latents.append(latents[SPLIT_TRAIN])\n",
    "    all_train_stim_ids = np.concatenate(all_train_stim_ids)\n",
    "    all_train_latents = np.concatenate(all_train_latents)\n",
    "    \n",
    "    unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "    unique_train_latents = all_train_latents[indices]\n",
    "    print('candidate set size: ', len(unique_stim_ids))\n",
    "\n",
    "for subj in SUBJECTS_ADDITIONAL_TEST:\n",
    "    print(subj)\n",
    "\n",
    "    stim_ids_test, _ = get_stim_info(subj, TEST_IMAGES)\n",
    "    stim_ids_imagery, _ =  get_stim_info(subj, SPLIT_IMAGERY_WEAK)\n",
    "\n",
    "    latents = get_latents_for_splits(subj, FEATS_CONFIG, [SPLIT_TRAIN, TEST_IMAGES, SPLIT_IMAGERY_WEAK], TRAINING_MODE)\n",
    "    latents = standardize_latents(latents)\n",
    "\n",
    "    predictions = load_predictions(BETAS_DIR, subj, TRAINING_MODE, FEATS_CONFIG, surface=SURFACE, mask=MASK)\n",
    "\n",
    "    pred_latents_imagery = predictions[SPLIT_IMAGERY_WEAK]\n",
    "    if len(RESTANDARDIZE_PREDS)>0:\n",
    "        print(f'standardizing imagery predictions with {len(refs)} refs ({RESTANDARDIZE_PREDS})')\n",
    "        refs = np.concatenate([predictions[split] for split in RESTANDARDIZE_PREDS])\n",
    "        transform = StandardScaler().fit(refs)\n",
    "        pred_latents_imagery = transform.transform(pred_latents_imagery)\n",
    "\n",
    "    all_preds.append(pred_latents_imagery)\n",
    "\n",
    "    # test_stim_ids_mod = results['stimulus_ids'][results['stimulus_types'] == IMAGE]\n",
    "    # test_latents_mod = results['latents'][results['stimulus_types'] == IMAGE]\n",
    "\n",
    "    if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "        # # account for case that sometimes both the image and the caption are part of the training set\n",
    "        # unique_stim_ids, indices = np.unique(stim_ids, return_index=True)\n",
    "        # unique_train_latents = train_latents[indices]\n",
    "        unique_stim_ids, indices = np.unique(all_train_stim_ids, return_index=True)\n",
    "        unique_train_latents = all_train_latents[indices]\n",
    "\n",
    "        candidate_latents = np.concatenate((latents[SPLIT_IMAGERY_WEAK], latents[TEST_IMAGES], unique_train_latents))\n",
    "        candidate_latent_ids = np.concatenate((stim_ids_imagery, stim_ids_test, unique_stim_ids))\n",
    "        print('candidate set size: ', len(candidate_latent_ids))\n",
    "    else:\n",
    "        candidate_latents = np.array(latents[SPLIT_IMAGERY_WEAK])\n",
    "        candidate_latent_ids = np.array(stim_ids_imagery)\n",
    "\n",
    "    # acc = analysis_ranking(pred_latents_imagery, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, SPLIT_IMAGERY_WEAK, N_SAMPLES, N_NEIGHBORS, out_file_name=f\"{IMAGERY}_weak_{TRAINING_MODE}_decoder_{subj}.png\", hspace=0.2)\n",
    "\n",
    "    all_pairwise_accs.append(acc)\n",
    "\n",
    "print(f'Mean pairwise acc: {np.mean(all_pairwise_accs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak imagery decoding averaged over subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_preds_avgd = np.mean(all_preds, axis=0)\n",
    "# acc = analysis_ranking(all_preds_avgd, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, SPLIT_IMAGERY_WEAK, N_SAMPLES, N_NEIGHBORS, out_file_name=f\"{IMAGERY}_weak_{TRAINING_MODE}_decoder_averaged.png\", hspace=0.2, subsample='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 5\n",
    "all_preds_avgd = np.mean(all_preds, axis=0)\n",
    "if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "    out_file_name = f\"{IMAGERY}_weak_{TRAINING_MODE}_decoder_averaged_large_candidate_set_best.png\"\n",
    "else:\n",
    "    out_file_name = f\"{IMAGERY}_weak_{TRAINING_MODE}_decoder_averaged_best.png\"\n",
    "acc = analysis_ranking(all_preds_avgd, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, SPLIT_IMAGERY_WEAK, N_SAMPLES, N_NEIGHBORS, out_file_name=out_file_name, hspace=0.2, wspace=0.2, subsample='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## worst samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WHOLE_TRAIN_AND_TEST_SET_AS_CANDIDATE_SET:\n",
    "    out_file_name = f\"{IMAGERY}_weak_{TRAINING_MODE}_decoder_averaged_large_candidate_set_worst.png\"\n",
    "else:\n",
    "    out_file_name = f\"{IMAGERY}_weak_{TRAINING_MODE}_decoder_averaged_worst.png\"\n",
    "acc = analysis_ranking(all_preds_avgd, stim_ids_imagery, candidate_latents, candidate_latent_ids, subj, SPLIT_IMAGERY_WEAK, N_SAMPLES, N_NEIGHBORS, out_file_name=out_file_name, hspace=0.2, wspace=0.2, subsample='worst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE for Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fmri_betas_full, train_stim_ids, train_stim_types = get_fmri_data(\n",
    "#     BETAS_DIR,\n",
    "#     SUBJECT,\n",
    "#     SPLIT_TRAIN,\n",
    "#     TRAINING_MODE,\n",
    "#     surface=SURFACE,\n",
    "# )\n",
    "# N_TRAIN_BETAS = 1000\n",
    "# train_paths = train_paths[np.random.choice(range(len(train_paths)), size=N_TRAIN_BETAS, replace=False)]\n",
    "\n",
    "# train_fmri_betas, test_fmri_betas, train_fmri_betas_standardized, test_fmri_betas_standardized = load_betas(train_paths, test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_betas(train_betas, test_betas, title, binwidth=3):\n",
    "#     X = np.concatenate((train_betas.flatten(), test_betas.flatten()))\n",
    "#     hue = ['train'] * train_betas.size + ['test'] * test_betas.size\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     sns.histplot(x=X, hue=hue, binwidth=binwidth)\n",
    "#     plt.title(title)\n",
    "\n",
    "# print(np.nanmean(train_fmri_betas.mean(axis=0)))\n",
    "# print(np.nanmean(test_fmri_betas.mean(axis=0)))\n",
    "# print(np.nanmean(train_fmri_betas_standardized.mean(axis=0)))\n",
    "# print(np.nanmean(test_fmri_betas_standardized.mean(axis=0)))\n",
    "\n",
    "# plot_betas(train_fmri_betas, test_fmri_betas, title='unstandardized')\n",
    "# plt.ylim(0, 10000000)\n",
    "# plt.xlim(-25, 25)\n",
    "\n",
    "# plot_betas(train_fmri_betas_standardized, test_fmri_betas, title='standardized', binwidth=0.3)\n",
    "# plt.ylim(0, 4000000)\n",
    "# plt.xlim(-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBJECT = 'sub-01'\n",
    "# MODEL = \"imagebind\"\n",
    "# SURFACE = True\n",
    "\n",
    "# # TRAINING_MODE = \"images\"\n",
    "# TRAINING_MODE = \"agnostic\"\n",
    "\n",
    "# BETAS_SUFFIX = 'betas'\n",
    "# BETAS_DIR = os.path.join(FMRI_DATA_DIR, BETAS_SUFFIX)\n",
    "\n",
    "# # feats = 'avg'\n",
    "# # test_feats = 'avg'\n",
    "# FEATS = 'default'\n",
    "# TEST_FEATS = 'default'\n",
    "# # feats = 'lang'\n",
    "# # test_feats = 'lang'\n",
    "# # vision_feats = 'vision_features_cls'\n",
    "# VISION_FEATS = 'default'\n",
    "# # vision_feats = 'n_a'\n",
    "\n",
    "# LANG_FEATS = 'default'\n",
    "# # lang_feats = 'lang_features_cls'\n",
    "# # lang_feats = 'lang_features_mean'\n",
    "\n",
    "# FEATS_CONFIG = LatentFeatsConfig(MODEL, FEATS, TEST_FEATS, VISION_FEATS, LANG_FEATS)\n",
    "\n",
    "\n",
    "# test_fmri_betas, test_stim_ids, test_stim_types = get_fmri_data(\n",
    "#     BETAS_DIR,\n",
    "#     SUBJECT,\n",
    "#     SPLIT_TEST,\n",
    "#     surface=SURFACE,\n",
    "# )\n",
    "# imagery_fmri_betas, imagery_stim_ids, imagery_stim_types = get_fmri_data(\n",
    "#     BETAS_DIR,\n",
    "#     SUBJECT,\n",
    "#     SPLIT_IMAGERY,\n",
    "#     surface=SURFACE,\n",
    "# )\n",
    "\n",
    "# train_latents = get_latent_features(FEATS_CONFIG, SUBJECT, SPLIT_TRAIN, mode=TRAINING_MODE)\n",
    "# test_latents = get_latent_features(FEATS_CONFIG, SUBJECT, SPLIT_TEST)\n",
    "# imagery_latents = get_latent_features(FEATS_CONFIG, SUBJECT, SPLIT_IMAGERY)\n",
    "\n",
    "# train_latents, test_latents, imagery_latents = standardize_latents(\n",
    "#     train_latents, test_latents, imagery_latents\n",
    "# )\n",
    "\n",
    "# results = load_predictions(BETAS_DIR, SUBJECT, TRAINING_MODE, FEATS_CONFIG, surface=SURFACE)\n",
    "\n",
    "# # stim_ids, stim_types, train_latents, gray_matter_mask, results, train_paths, test_paths = load(BETAS_DIR, MODEL, SUBJECT, MODE, FEATS_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_latents_tsne(train_lat, test_lat, pred_lat, imagery_pred_lat, title, train_subset=1000):\n",
    "#     train_latents_subset = train_latents[np.random.choice(range(len(train_lat)), size=train_subset, replace=False)]\n",
    "    \n",
    "#     tsne = TSNE(n_components=2, learning_rate='auto', verbose=1, n_jobs=10, n_iter=1000)\n",
    "#     X_embedded = tsne.fit_transform(np.concatenate((train_latents_subset, test_lat, pred_lat, imagery_pred_lat)))\n",
    "    \n",
    "#     print(X_embedded.shape)\n",
    "#     assert X_embedded.shape[1] == 2\n",
    "#     hue = ['train'] * len(train_latents_subset) + ['test'] * len(test_lat) + ['predictions'] * len(pred_lat) + ['imagery_predictions'] * len(imagery_pred_lat)\n",
    "#     # alphas = [0.3] * len(train_latents_subset) + [1] * len(test_lat) + [1] * len(preds)\n",
    "    \n",
    "#     plt.figure(figsize=(20, 12))\n",
    "#     sns.scatterplot(\n",
    "#         x = X_embedded[:, 0], y = X_embedded[:, 1],\n",
    "#         hue = hue,\n",
    "#         alpha = 0.8\n",
    "#     )\n",
    "#     plt.title(title)\n",
    "\n",
    "# plot_latents_tsne(train_latents, results['latents'], results['predictions'], results['imagery_predictions'], title=\"not standardized\")\n",
    "\n",
    "# pred_latents_standardized = StandardScaler().fit_transform(results['predictions'])   \n",
    "# imagery_pred_latents_standardized = StandardScaler().fit_transform(results['imagery_predictions'])\n",
    "# plot_latents_tsne(train_latents, results['latents'], pred_latents_standardized, imagery_pred_latents_standardized, \"standardized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE for Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_betas_tsne(train_betas, test_betas, title, train_subset=None):\n",
    "#     if train_subset is not None:\n",
    "#         train_betas_subset = train_betas[np.random.choice(range(len(train_betas)), size=train_subset, replace=False)]\n",
    "#     else:\n",
    "#         train_betas_subset = train_betas\n",
    "#     train_test = np.concatenate((train_betas_subset, test_betas))\n",
    "#     tsne = TSNE(n_components=2, learning_rate='auto', verbose=1, n_jobs=10, n_iter=1000)\n",
    "#     X_embedded = tsne.fit_transform(train_test)\n",
    "    \n",
    "#     print(X_embedded.shape)\n",
    "#     assert X_embedded.shape[1] == 2\n",
    "#     hue = ['train'] * len(train_betas_subset) + ['test'] * len(test_betas)\n",
    "#     # alpha = [1] * len(test_betas) + [0.3] * len(train_betas_subset)\n",
    "    \n",
    "#     plt.figure(figsize=(20, 12))\n",
    "#     sns.scatterplot(\n",
    "#         x = X_embedded[:, 0], y = X_embedded[:, 1],\n",
    "#         hue = hue,\n",
    "#         # alpha = alpha\n",
    "#     )\n",
    "#     plt.title(title)\n",
    "\n",
    "# plot_betas_tsne(train_fmri_betas, test_fmri_betas, title=\"not standardized\")\n",
    "# plot_betas_tsne(train_fmri_betas_standardized, test_fmri_betas_standardized, \"standardized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  },
  "vscode": {
   "interpreter": {
    "hash": "64c2261fd1335a391d209058834a77a3cc43bbc1dadc63860e2129a303b1f182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
