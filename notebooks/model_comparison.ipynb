{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T10:31:26.216288860Z",
     "start_time": "2023-12-12T10:31:26.209251348Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from PIL import ImageColor\n",
    "import matplotlib.colors\n",
    "\n",
    "from utils import FEATURES_DIR, RESULTS_DIR, SUBJECTS, NUM_TEST_STIMULI\n",
    "from analyses.ridge_regression_decoding import NUM_CV_SPLITS, DECODER_OUT_DIR, calc_rsa, calc_rsa_images, calc_rsa_captions, get_fmri_data, pairwise_accuracy\n",
    "from notebook_utils import load_results_data, add_avg_subject, create_result_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_results_data()\n",
    "\n",
    "MODEL_ORDER = [\"random-flava\", \"vit-b-16\", \"vit-l-16\", \"resnet-18\", \"resnet-50\", \"resnet-152\", \"dino-base\", \"dino-large\", \"dino-giant\",\n",
    "               \"bert-base-uncased\", \"bert-large-uncased\", \"llama2-7b\", \"llama2-13b\", \"mistral-7b\", \"mixtral-8x7b\", \"gpt2-small\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\",\n",
    "               \"visualbert\", \"bridgetower-large\", \"clip\", \"flava\", \"imagebind\", \"lxmert\", \"vilt\"]\n",
    "\n",
    "all_data = all_data[all_data.model.isin(MODEL_ORDER)]\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_avg = add_avg_subject(all_data)\n",
    "all_data_avg = all_data_avg[all_data_avg.subject == \"average\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref performance order:\n",
    "#['random-flava', 'resnet-152', 'vit-b-16', 'vit-l-16', 'resnet-18', 'resnet-50', 'dino-giant', 'dino-large', 'dino-base', 'bert-large-uncased', 'bert-base-uncased',\n",
    "# 'llama2-7b', 'mixtral-8x7b', 'mistral-7b', 'llama2-13b', 'gpt2-small', 'gpt2-medium', 'gpt2-xl', 'gpt2-large', 'visualbert', 'bridgetower-large', 'clip', 'flava', 'imagebind', 'lxmert', 'vilt']\n",
    "FEAT_OPTIONS = [\"vision\", \"lang\", \"vision+lang\", \"matched\"]\n",
    "\n",
    "def calc_model_feat_order(data):\n",
    "    all_model_feats = data.model_feat.unique()\n",
    "    all_models = data.model.unique()\n",
    "    for model in all_models:\n",
    "        if model not in MODEL_ORDER:\n",
    "            raise RuntimeError(f\"Model missing in order: {model}\")\n",
    "    model_feat_order = []\n",
    "    for model in MODEL_ORDER:\n",
    "        for feats in FEAT_OPTIONS:\n",
    "            model_feat = f\"{model}_{feats}\"\n",
    "            if model_feat in all_model_feats:\n",
    "                model_feat_order.append(model_feat)\n",
    "\n",
    "    return model_feat_order\n",
    "\n",
    "for model in MODEL_ORDER:\n",
    "    print(model, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_metric = 'cosine'\n",
    "METRICS = [f'acc_{distance_metric}', f'acc_{distance_metric}_captions', f'acc_{distance_metric}_images']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modality-agnostic decoding vs. modality-specific decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_models = all_data_avg[all_data_avg.features == \"vision+lang\"].model.unique().tolist()\n",
    "MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY = [m+'_lang' for m in multimodal_models] + [m+'_vision' for m in multimodal_models]\n",
    "MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY += [m+'_matched' for m in multimodal_models]\n",
    "\n",
    "vision_models = [m for m in all_data_avg[all_data_avg.features == \"vision\"].model.unique() if len(all_data_avg[all_data_avg.model == m].features.unique()) == 1]\n",
    "for m in multimodal_models:\n",
    "    print(m, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = all_data_avg\n",
    "\n",
    "MODEL_FEATS_EXCLUDED = [\"bridgetower-large_multi\", \"random-flava_vision\", \"random-flava_lang\"] + MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY\n",
    "dp = dp[~dp.model_feat.isin(MODEL_FEATS_EXCLUDED)].copy()\n",
    "dp = dp[dp[\"mask\"] == \"whole_brain\"]\n",
    "\n",
    "model_order = ['random-flava']\n",
    "for features in [\"vision\", \"lang\", \"vision+lang\"]:\n",
    "    model_scores = []\n",
    "    data_feats = dp[(dp.features == features) & (dp.model != 'random-flava')]\n",
    "    models = data_feats.model.unique()\n",
    "    for model in models:\n",
    "        dp_model = dp[(dp.model == model) & (dp.training_mode == 'modality-agnostic')]\n",
    "\n",
    "        dp_model = dp_model[(dp_model.vision_features == 'visual_feature_mean')]\n",
    "        \n",
    "        dp_model_caps = dp_model[dp_model.metric == \"pairwise_acc_captions\"]\n",
    "        assert len(dp_model_caps) == 6, f\"{model}: {len(dp_model_caps)} entries\"\n",
    "        # print(f\"pairwise_acc_captions: {dp_model_caps.value.mean():.2f}\")\n",
    "        dp_model_imgs = dp_model[dp_model.metric == \"pairwise_acc_images\"]\n",
    "        assert len(dp_model_imgs) == 6, len(dp_model_imgs)\n",
    "        # print(f\"pairwise_acc_images: {dp_model_imgs.value.mean():.2f}\")\n",
    "        avg = np.mean([dp_model_caps.value.mean(), dp_model_imgs.value.mean()])\n",
    "        print(f\"{model} pairwise_acc mean : {dp_model[dp_model.metric == 'pairwise_acc_mean'].value.mean():.3f} std: {dp_model[dp_model.metric == 'pairwise_acc_mean'].value.std():.3f}\")\n",
    "        model_scores.append(avg)\n",
    "        \n",
    "    model_order.extend([model for _, model in sorted(zip(model_scores, models), key=lambda pair: pair[0])])\n",
    "print(model_order)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.6)\n",
    "TRAIN_MODE_ORDER = [\"images\", \"captions\", \"modality-agnostic\"]\n",
    "FEAT_ORDER = [\"vision models\", \"language models\", \"multimodal models\"]\n",
    "\n",
    "MODEL_FEATS_EXCLUDED = [\"bridgetower-large_multi\", \"random-flava_vision\", \"random-flava_lang\"] + MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY\n",
    "data_to_plot = all_data_avg[~all_data_avg.model_feat.isin(MODEL_FEATS_EXCLUDED)].copy()\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot.vision_features == 'visual_feature_mean']\n",
    "\n",
    "data_to_plot[\"features\"] = data_to_plot.features.replace({\"vision\": \"vision models\", \"lang\": \"language models\", \"vision+lang\": \"multimodal models\"})\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot[\"mask\"] == \"whole_brain\"]\n",
    "model_feat_order = calc_model_feat_order(data_to_plot)\n",
    "\n",
    "metrics_order = [\"pairwise_acc_captions\", \"pairwise_acc_images\", \"pairwise_acc_mean\"]\n",
    "figure, lgd = create_result_graph(data_to_plot, model_feat_order, metrics=metrics_order, row_order=metrics_order, hue_order=FEAT_ORDER, ylim=(0.5, 1),\n",
    "                             legend_bbox=(0.06,0.99), height=4.5, legend_title=\"Modality-agnostic decoders projecting into feature space of\")\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"features_comparison_pairwise_acc.png\"), bbox_extra_artists=(lgd,), bbox_inches='tight', pad_inches=0, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V, L, or multimodal feats for multimodal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_legend = {\"vision\": \"vision feature space\", \"lang\": \"language feature space\", \"vision+lang\": \"multimodal feature space: concat\", \"matched\": \"matched feature space\"}\n",
    "feat_order = [\"vision\", \"lang\", \"vision+lang\", \"matched\"]\n",
    "feat_order = [feat_legend[feat] for feat in feat_order]\n",
    "\n",
    "model_feats_exluded = [\"bridgetower-large_multi\", \"random-flava_vision\", \"random-flava_lang\", \"random-flava_vision+lang\"]\n",
    "data_to_plot = all_data_avg[~all_data_avg.model_feat.isin(model_feats_exluded)].copy()\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot.model.isin(multimodal_models)]\n",
    "data_to_plot = data_to_plot[data_to_plot.vision_features == 'visual_feature_mean']\n",
    "\n",
    "data_to_plot[\"features\"] = data_to_plot.features.replace(feat_legend)\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot[\"mask\"] == \"whole_brain\"]\n",
    "model_feat_order = calc_model_feat_order(data_to_plot)\n",
    "\n",
    "metrics_order = [\"pairwise_acc_captions\", \"pairwise_acc_images\", \"pairwise_acc_mean\"]\n",
    "figure, lgd = create_result_graph(data_to_plot, model_feat_order, metrics=metrics_order, row_order=metrics_order, hue_order=feat_order, ylim=(0.5, 1),\n",
    "                             legend_bbox=(0.06,1.01), height=4.5, legend_title=\"Modality-agnostic decoders projecting into\")\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"features_comparison_pairwise_acc_multimodal_models.png\"), bbox_extra_artists=(lgd,), bbox_inches='tight', pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLS or mean feats for vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feats_exluded = [\"bridgetower-large_multi\", \"random-flava_vision\", \"random-flava_lang\", \"random-flava_vision+lang\"]\n",
    "data_to_plot = all_data_avg[~all_data_avg.model_feat.isin(model_feats_exluded)].copy()\n",
    "\n",
    "vision_models_both_options = [m for m in vision_models if len(data_to_plot[data_to_plot.model == m].vision_features.unique()) == 2]\n",
    "data_to_plot = data_to_plot[data_to_plot.model.isin(vision_models_both_options)]\n",
    "\n",
    "vision_feat_order = [\"Mean vision features\", \"CLS token vision features\"]\n",
    "data_to_plot[\"vision_features\"] = data_to_plot.vision_features.replace({\"visual_feature_mean\": \"Mean vision features\", \"visual_feature_cls\": \"CLS token vision features\"})\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot.training_mode == \"modality-agnostic\"]\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot[\"mask\"] == \"whole_brain\"]\n",
    "model_feat_order = calc_model_feat_order(data_to_plot)\n",
    "\n",
    "metrics_order = [\"pairwise_acc_captions\", \"pairwise_acc_images\", \"pairwise_acc_mean\"]\n",
    "figure, lgd = create_result_graph(data_to_plot, model_feat_order, metrics=metrics_order, row_order=metrics_order, hue_variable=\"vision_features\", hue_order=vision_feat_order, ylim=(0.5, 1),\n",
    "                             legend_bbox=(0.06,1.0), height=4.5, legend_title=\"Modality-agnostic decoders projecting into\", plot_modality_specific=False)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"features_comparison_pairwise_acc_vision_models.png\"), bbox_extra_artists=(lgd,), bbox_inches='tight', pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI-based decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_ORDER = [\"high-level visual ROI\", \"low-level visual ROI\", \"language ROI\"]\n",
    "# MASK_ORDER = [\"high-level visual ROI\", \"low-level visual ROI\", \"language ROI\", \"functional_Language\", \"functional_Visual1\", \"functional_Visual2\", \"functional_Default\", \"functional_Visual1_Visual2\"]\n",
    "\n",
    "MASK_PALETTE = sns.color_palette('Set2')[3:3+len(MASK_ORDER)][::-1]\n",
    "\n",
    "# MODEL_FEATS_INCLUDED = [\"resnet-50_vision\", \"dino-giant_vision\", \"gpt2-large_lang\", \"llama2-13b_lang\", \"flava_vision+lang\"]\n",
    "# data_all_masks = all_data_avg[all_data_avg.model_feat.isin(MODEL_FEATS_INCLUDED)].copy()\n",
    "\n",
    "MODEL_FEATS_EXCLUDED = [\"bridgetower-large_multi\", \"random-flava_vision\", \"random-flava_lang\"] + MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY\n",
    "data_all_masks = all_data_avg[~all_data_avg.model_feat.isin(MODEL_FEATS_EXCLUDED)].copy()\n",
    "\n",
    "data_all_masks = data_all_masks[data_all_masks.vision_features == 'visual_feature_mean']\n",
    "\n",
    "\n",
    "data_all_masks[\"mask\"] = data_all_masks[\"mask\"].replace({\"anatomical_visual_low_level\": \"low-level visual ROI\", \"anatomical_lang\": \"language ROI\", \"anatomical_visual_high_level\": \"high-level visual ROI\"})\n",
    "\n",
    "data_all_masks = data_all_masks[data_all_masks[\"mask\"].isin(MASK_ORDER)].copy()\n",
    "\n",
    "model_feat_order = calc_model_feat_order(data_all_masks)\n",
    "\n",
    "metrics_order = [\"pairwise_acc_captions\", \"pairwise_acc_images\", \"pairwise_acc_mean\"]\n",
    "\n",
    "dodge = 0.47\n",
    "# dodge = 0.6\n",
    "figure, lgd = create_result_graph(data_all_masks, model_feat_order, metrics=metrics_order, row_order=metrics_order, hue_variable=\"mask\", hue_order=MASK_ORDER, palette=MASK_PALETTE, ylim=(0.5, 1),\n",
    "                                  legend_title=\"Modality-agnostic decoders trained on fMRI data from\", dodge=dodge, legend_bbox=(0.06,0.99))\n",
    "\n",
    "colors_bg = sns.color_palette('Set2')[:3]\n",
    "for i in range(len(figure.axes)):\n",
    "    figure.axes[i, 0].axvspan(-0.5, 0.5, facecolor=colors_bg[2], alpha=0.2, zorder=-100)\n",
    "    figure.axes[i, 0].axvspan(0.5, 8.5, facecolor=colors_bg[0], alpha=0.2, zorder=-100)\n",
    "    figure.axes[i, 0].axvspan(8.5, 18.5, facecolor=colors_bg[1], alpha=0.2, zorder=-100)\n",
    "    figure.axes[i, 0].axvspan(18.5, 25.5, facecolor=colors_bg[2], alpha=0.2, zorder=-100)\n",
    "plt.xlim((-0.5, 25.5))\n",
    "\n",
    "\n",
    "# plt.subplots_adjust(top=0.98, bottom=0.05, hspace=0)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"roi_comparison_pairwise_acc.png\"), bbox_extra_artists=(lgd,), bbox_inches='tight', pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_voxels_data = data_all_masks[~data_all_masks.num_voxels.isna()]\n",
    "n_voxels_data = {mask: n_voxels_data[n_voxels_data[\"mask\"] == mask].num_voxels.mean() for mask in n_voxels_data[\"mask\"].unique()}\n",
    "# n_voxels_data.update({\n",
    "#     # \"whole_brain\": 214739,\n",
    "#     \"visual_high_level\": 14698,\n",
    "#     \"visual_low_level\": 13955\n",
    "# })\n",
    "print(n_voxels_data)\n",
    "# sns.barplot(data=n_voxels_data)\n",
    "# # plt.yscale(\"log\")\n",
    "# plt.xticks(rotation = 80)\n",
    "# plt.ylabel(\"num voxels\")\n",
    "# plt.title(\"Number of voxels for each mask (whole brain: 214,739)\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(RESULTS_DIR, f\"num_voxels.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-subject results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_ORDER = [\"vision\", \"lang\", \"vision+lang\"]\n",
    "FEAT_PALETTE = sns.color_palette('Set2')[:3]\n",
    "\n",
    "def create_result_graph_all_subjs(data, model_feat_order, metrics=[\"pairwise_acc_captions\", \"pairwise_acc_images\"], hue_variable=\"features\", hue_order=FEAT_ORDER, ylim=None,\n",
    "                        legend_title=\"Modality-agnostic decoders based on features from\", palette=FEAT_PALETTE, dodge=False, noise_ceilings=None, plot_modality_specific=True,\n",
    "                       row_variable=\"metric\", col_variable=None):\n",
    "    data_training_mode_full = data[data.training_mode == \"modality-agnostic\"]\n",
    "\n",
    "\n",
    "    assert len(data_training_mode_full[(data_training_mode_full.model_feat == 'gpt2-large_lang') & (data_training_mode_full.metric == metrics[0]) & (data_training_mode_full[hue_variable] == data_training_mode_full[hue_variable].values[0])]) == 6\n",
    "\n",
    "    catplot_g, data_plotted, lgd = plot_metric_catplot(data_training_mode_full, order=model_feat_order, metrics=metrics, x_variable=\"model_feat\", legend_title=legend_title, aspect=2, legend_bbox=(0,1.05), rotation=89, cut_labels=False,\n",
    "                                                  hue_variable=hue_variable, row_variable=row_variable, col_variable=col_variable, hue_order=hue_order, palette=palette, ylim=ylim, noise_ceilings=noise_ceilings)\n",
    "\n",
    "    if plot_modality_specific:\n",
    "        for i, subj in zip(range(6), SUBJECTS):\n",
    "            data_subj = data[data.subject == subj]\n",
    "            data_training_mode_captions = data_subj[data_subj.training_mode == \"captions\"]\n",
    "            data_training_mode_images = data_subj[data_subj.training_mode == \"images\"]\n",
    "            _ = plot_metric(data_training_mode_captions, kind=\"point\", order=model_feat_order, metric=metrics[0], x_variable=\"model_feat\", dodge=dodge,\n",
    "                                          hue_variable=hue_variable, hue_order=hue_order, palette=[(0, 0, 0)], axis=catplot_g.axes[i, 0], marker=\"o\", plot_legend=False, ylim=ylim)\n",
    "            g, _ = plot_metric(data_training_mode_images, kind=\"point\", order=model_feat_order, metric=metrics[0], x_variable=\"model_feat\", dodge=dodge,\n",
    "                                          hue_variable=hue_variable, hue_order=hue_order, palette=[(0, 0, 0)], axis=catplot_g.axes[i, 0], marker=\"x\", plot_legend=False, ylim=ylim)\n",
    "            \n",
    "            _ = plot_metric(data_training_mode_captions, kind=\"point\", order=model_feat_order, metric=metrics[1], x_variable=\"model_feat\", dodge=dodge,\n",
    "                                          hue_variable=hue_variable, hue_order=hue_order, palette=[(0, 0, 0)], axis=catplot_g.axes[i, 1], marker=\"o\", plot_legend=False, ylim=ylim)\n",
    "            _ = plot_metric(data_training_mode_images, kind=\"point\", order=model_feat_order, metric=metrics[1], x_variable=\"model_feat\", dodge=dodge,\n",
    "                                          hue_variable=hue_variable, hue_order=hue_order, palette=[(0, 0, 0)], axis=catplot_g.axes[i, 1], marker=\"x\", plot_legend=False, ylim=ylim)\n",
    "        \n",
    "        handles, labels = g.get_legend_handles_labels()\n",
    "        new_labels = [\"captions\", \"images\"]\n",
    "        new_handles = [handles[0], handles[-1]]\n",
    "        catplot_g.fig.legend(handles=new_handles, labels=new_labels, ncol=2, title=\"Modality-specific decoders trained on\", loc='upper right')\n",
    "\n",
    "        for i in range(6):\n",
    "            catplot_g.axes[i,0].set_title(f\"subject {i+1} | captions\", fontsize=25)\n",
    "            catplot_g.axes[i,1].set_title(f\"subject {i+1} | images\", fontsize=25)\n",
    "\n",
    "            catplot_g.axes[i,0].set_ylabel('pairwise accuracy')\n",
    "\n",
    "    return catplot_g, lgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE_ORDER = [\"images\", \"captions\", \"modality-agnostic\"]\n",
    "FEAT_ORDER = [\"vision model\", \"language model\", \"multimodal model\"]\n",
    "\n",
    "MODEL_FEATS_EXCLUDED = [\"bridgetower-large_multi\", \"random-flava_vision\", \"random-flava_lang\"] + MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY\n",
    "data_to_plot = all_data[~all_data.model_feat.isin(MODEL_FEATS_EXCLUDED)].copy()\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot.vision_features == 'visual_feature_mean']\n",
    "\n",
    "data_to_plot[\"features\"] = data_to_plot.features.replace({\"vision\": \"vision model\", \"lang\": \"language model\", \"vision+lang\": \"multimodal model\"})\n",
    "\n",
    "data_to_plot = data_to_plot[data_to_plot[\"mask\"] == \"whole_brain\"]\n",
    "model_feat_order = calc_model_feat_order(data_to_plot)\n",
    "\n",
    "figure, lgd = create_result_graph_all_subjs(data_to_plot, model_feat_order, metrics=[\"pairwise_acc_captions\", \"pairwise_acc_images\"], hue_order=FEAT_ORDER, ylim=(0.5, 1), row_variable=\"subject\", col_variable=\"metric\")\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"features_comparison_pairwise_acc_per_subject.png\"), dpi=300, bbox_extra_artists=(lgd,), bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "64c2261fd1335a391d209058834a77a3cc43bbc1dadc63860e2129a303b1f182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
