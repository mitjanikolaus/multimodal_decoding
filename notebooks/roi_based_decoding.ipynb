{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T10:31:26.216288860Z",
     "start_time": "2023-12-12T10:31:26.209251348Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from PIL import ImageColor\n",
    "import matplotlib.colors\n",
    "\n",
    "from utils import FEATURES_DIR, RESULTS_DIR, SUBJECTS, NUM_TEST_STIMULI\n",
    "from analyses.ridge_regression_decoding import NUM_CV_SPLITS, DECODER_OUT_DIR, calc_rsa, calc_rsa_images, calc_rsa_captions, get_fmri_data, pairwise_accuracy, ACC_MODALITY_AGNOSTIC, ACC_CAPTIONS, ACC_IMAGES, get_default_features, get_default_vision_features\n",
    "from notebook_utils import add_avg_subject, create_result_graph, plot_metric_catplot, plot_metric, load_results_data, ACC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 159/5332 [00:13<09:32,  9.03it/s]"
     ]
    }
   ],
   "source": [
    "all_data = load_results_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ORDER = [\"random-flava\", \"vit-b-16\", \"vit-l-16\", \"resnet-18\", \"resnet-50\", \"resnet-152\", \"dino-base\", \"dino-large\", \"dino-giant\",\n",
    "               \"bert-base-uncased\", \"bert-large-uncased\", \"llama2-7b\", \"llama2-13b\", \"mistral-7b\", \"mixtral-8x7b\", \"gpt2-small\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\",\n",
    "               \"visualbert\", \"bridgetower-large\", \"clip\", \"flava\", \"imagebind\", \"vilt\"]\n",
    "\n",
    "all_data = all_data[all_data.model.isin(MODEL_ORDER)]\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FEAT_OPTIONS = [\"vision\", \"lang\", \"matched\"]\n",
    "\n",
    "def calc_model_feat_order(data, feat_options=DEFAULT_FEAT_OPTIONS):\n",
    "    all_model_feats = data.model_feat.unique()\n",
    "    all_models = data.model.unique()\n",
    "    for model in all_models:\n",
    "        if model not in MODEL_ORDER:\n",
    "            raise RuntimeError(f\"Model missing in order: {model}\")\n",
    "    model_feat_order = []\n",
    "    for model in MODEL_ORDER:\n",
    "        for feats in feat_options:\n",
    "            model_feat = f\"{model}_{feats}\"\n",
    "            if model_feat in all_model_feats:\n",
    "                model_feat_order.append(model_feat)\n",
    "\n",
    "    return model_feat_order\n",
    "\n",
    "multimodal_models = all_data[all_data.features.isin([\"matched\", \"fused_mean\", \"fused_cls\"])].model.unique().tolist()\n",
    "multimodal_models\n",
    "# for model in MODEL_ORDER:\n",
    "#     print(model, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_default_feats = all_data.copy()\n",
    "for model in all_data.model.unique():\n",
    "    default_feats = get_default_features(model)\n",
    "    default_vision_feats = get_default_vision_features(model)\n",
    "    data_default_feats = data_default_feats[((data_default_feats.model == model) & (data_default_feats.features == default_feats) & (data_default_feats.vision_features == default_vision_feats)) | (data_default_feats.model != model)]\n",
    "    \n",
    "data_default_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY = [m+'_lang' for m in multimodal_models] + [m+'_vision' for m in multimodal_models]\n",
    "MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY += [m+'_concat' for m in multimodal_models]\n",
    "\n",
    "vision_models = [m for m in all_data[all_data.features == \"vision\"].model.unique() if len(all_data[all_data.model == m].features.unique()) == 1]\n",
    "for m in multimodal_models:\n",
    "    print(m, end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = ['random-flava']\n",
    "for features in DEFAULT_FEAT_OPTIONS:\n",
    "    print(features)\n",
    "    dp = data_default_feats.copy()\n",
    "    dp = dp[dp.features == features]\n",
    "    dp = dp[dp[\"mask\"] == \"whole_brain\"]\n",
    "    dp = dp[dp.training_mode == 'modality-agnostic']\n",
    "    \n",
    "    dp = dp[dp.metric == ACC_MODALITY_AGNOSTIC]\n",
    "    for model in dp.model.unique():\n",
    "        if len(dp[dp.model == model]) != len(SUBJECTS):\n",
    "            print(f\"unexpected number of datapoints for {model}: {len(dp[dp.model == model])}\")\n",
    "    scores = dp.groupby(\"model\").value.mean().sort_values()\n",
    "    print(scores)\n",
    "    model_order.extend(scores.index.values)\n",
    "    \n",
    "model_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI-based decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_ORDER = [\"high-level visual ROI\", \"low-level visual ROI\", \"language ROI\"]\n",
    "\n",
    "MASK_PALETTE = sns.color_palette('Set2')[3:3+len(MASK_ORDER)][::-1]\n",
    "\n",
    "# data_all_masks = all_data[all_data.model_feat.isin(MODEL_FEATS_INCLUDED)].copy()\n",
    "\n",
    "MODEL_FEATS_EXCLUDED = [\"bridgetower-large_multi\", \"random-flava_vision\", \"random-flava_lang\"] + MODEL_FEAT_MULTIMODAL_SINGLE_MODALITY\n",
    "data_all_masks = all_data[~all_data.model_feat.isin(MODEL_FEATS_EXCLUDED)].copy()\n",
    "\n",
    "data_all_masks = data_all_masks[data_all_masks.vision_features == 'visual_feature_mean']\n",
    "\n",
    "\n",
    "data_all_masks[\"mask\"] = data_all_masks[\"mask\"].replace({\"anatomical_visual_low_level\": \"low-level visual ROI\", \"anatomical_lang\": \"language ROI\", \"anatomical_visual_high_level\": \"high-level visual ROI\"})\n",
    "\n",
    "data_all_masks = data_all_masks[data_all_masks[\"mask\"].isin(MASK_ORDER)].copy()\n",
    "\n",
    "model_feat_order = calc_model_feat_order(data_all_masks)\n",
    "\n",
    "metrics_order = [\"pairwise_acc_captions\", \"pairwise_acc_images\", ACC_MEAN]\n",
    "\n",
    "dodge = 0.47\n",
    "# dodge = 0.6\n",
    "figure, lgd = create_result_graph(data_all_masks, model_feat_order, metrics=metrics_order, row_order=metrics_order, hue_variable=\"mask\", hue_order=MASK_ORDER, palette=MASK_PALETTE, ylim=(0.5, 1),\n",
    "                                  legend_title=\"Modality-agnostic decoders trained on fMRI data from\", dodge=dodge, legend_bbox=(0.06,0.99))\n",
    "\n",
    "colors_bg = sns.color_palette('Set2')[:3]\n",
    "for i in range(len(figure.axes)):\n",
    "    figure.axes[i, 0].axvspan(-0.5, 0.5, facecolor=colors_bg[2], alpha=0.2, zorder=-100)\n",
    "    figure.axes[i, 0].axvspan(0.5, 8.5, facecolor=colors_bg[0], alpha=0.2, zorder=-100)\n",
    "    figure.axes[i, 0].axvspan(8.5, 18.5, facecolor=colors_bg[1], alpha=0.2, zorder=-100)\n",
    "    figure.axes[i, 0].axvspan(18.5, 25.5, facecolor=colors_bg[2], alpha=0.2, zorder=-100)\n",
    "plt.xlim((-0.5, 25.5))\n",
    "\n",
    "\n",
    "# plt.subplots_adjust(top=0.98, bottom=0.05, hspace=0)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f\"roi_comparison_pairwise_acc.png\"), bbox_extra_artists=(lgd,), bbox_inches='tight', pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_voxels_data = data_all_masks[~data_all_masks.num_voxels.isna()]\n",
    "n_voxels_data = {mask: n_voxels_data[n_voxels_data[\"mask\"] == mask].num_voxels.mean() for mask in n_voxels_data[\"mask\"].unique()}\n",
    "# n_voxels_data.update({\n",
    "#     # \"whole_brain\": 214739,\n",
    "#     \"visual_high_level\": 14698,\n",
    "#     \"visual_low_level\": 13955\n",
    "# })\n",
    "print(n_voxels_data)\n",
    "# sns.barplot(data=n_voxels_data)\n",
    "# # plt.yscale(\"log\")\n",
    "# plt.xticks(rotation = 80)\n",
    "# plt.ylabel(\"num voxels\")\n",
    "# plt.title(\"Number of voxels for each mask (whole brain: 214,739)\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(RESULTS_DIR, f\"num_voxels.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "64c2261fd1335a391d209058834a77a3cc43bbc1dadc63860e2129a303b1f182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
