{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T10:31:26.216288860Z",
     "start_time": "2023-12-12T10:31:26.209251348Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from PIL import ImageColor, Image\n",
    "import matplotlib.colors\n",
    "\n",
    "from utils import NN_FEATURES_DIR, RESULTS_DIR, SUBJECTS, NUM_TEST_STIMULI, FMRI_SURFACE_LEVEL_DIR, STIM_INFO_PATH, COCO_IMAGES_DIR, STIMULI_IDS_PATH\n",
    "from analyses.ridge_regression_decoding import NUM_CV_SPLITS, RIDGE_DECODER_OUT_DIR, calc_rsa, calc_rsa_images, calc_rsa_captions, get_fmri_data, pairwise_accuracy, \\\n",
    "ACC_MODALITY_AGNOSTIC, ACC_CAPTIONS, ACC_IMAGES, ACC_CROSS_IMAGES_TO_CAPTIONS, ACC_CROSS_CAPTIONS_TO_IMAGES, ACC_IMAGERY, ACC_IMAGERY_WHOLE_TEST, get_default_features, get_default_vision_features, get_default_lang_features, Standardize, IMAGE, CAPTION, get_distance_matrix, dist_mat_to_pairwise_acc, get_fmri_data_paths, get_nn_latent_data\n",
    "\n",
    "from notebook_utils import add_avg_subject, create_result_graph, plot_metric_catplot, plot_metric, load_results_data, ACC_MEAN, ACC_CROSS_MEAN, PALETTE_BLACK_ONLY, METRICS_ERROR_ANALYSIS, get_data_default_feats, METRICS_BASE\n",
    "\n",
    "from feature_extraction.feat_extraction_utils import CoCoDataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors of test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:07<00:00, 71.18it/s]\n"
     ]
    }
   ],
   "source": [
    "MODELS = [\"imagebind\"] # imagebind gpt2-large blip2\n",
    "all_data = load_results_data(MODELS, metrics=METRICS_BASE+METRICS_ERROR_ANALYSIS, recompute_acc_scores=False)\n",
    "data_default_feats = get_data_default_feats(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:33<00:00,  5.56s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL = MODELS[0]\n",
    "df_model = data_default_feats[data_default_feats.model == MODEL]\n",
    "\n",
    "train_latents = dict()\n",
    "train_stim_ids = dict()\n",
    "for subject in tqdm(SUBJECTS):\n",
    "    assert len(df_model.vision_features.unique()) == 1\n",
    "    features = df_model.features.values[0]\n",
    "    # print(features)\n",
    "    vision_features = df_model.vision_features.values[0]\n",
    "    # print(vision_features)\n",
    "    lang_features = df_model.lang_features.values[0]\n",
    "    # print(lang_features)\n",
    "    _, stim_ids, stim_types = get_fmri_data_paths(subject, \"train\")\n",
    "    unique_train_stim_ids, idx_unique = np.unique(stim_ids, return_index=True)\n",
    "    train_stim_ids[subject] = unique_train_stim_ids\n",
    "    latents, _ = get_nn_latent_data(MODEL, features,\n",
    "                                    vision_features,\n",
    "                                    lang_features,\n",
    "                                    stim_ids,\n",
    "                                    stim_types,\n",
    "                                    subject,\n",
    "                                    \"train\"\n",
    "                                    )\n",
    "    train_latents[subject] = latents[idx_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_ds = CoCoDataset(COCO_IMAGES_DIR, STIM_INFO_PATH, STIMULI_IDS_PATH, 'both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HEIGHT = 500\n",
    "\n",
    "def display_stimuli(coco_ids):\n",
    "    # print(coco_ids)\n",
    "    print(\"nearest neighbor captions: \")\n",
    "    for coco_id in coco_ids:\n",
    "        print(coco_ds.get_stimuli_by_coco_id(coco_id)[1], end=\"\\n\")\n",
    "\n",
    "    print(\"nearest neighbor images: \")\n",
    "    imgs = [np.array(coco_ds.get_img_by_coco_id(img_id)) for img_id in coco_ids]\n",
    "    # min_height = np.min([np.array(im).shape[0] for im in imgs])\n",
    "    imgs = [np.vstack((img, np.repeat(255, max(0, MAX_HEIGHT - img.shape[0]) * img.shape[1]* img.shape[2]).reshape((max(0, MAX_HEIGHT - img.shape[0]), img.shape[1], img.shape[2])).astype(img.dtype)))[:MAX_HEIGHT,:,:] for img in imgs]\n",
    "    stacked = np.hstack(imgs)\n",
    "    img = Image.fromarray(stacked)\n",
    "    display(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('/home/mitja/data/multimodal_decoding/decoders/images/sub-01/imagebind_avg_test_avg_vision_features_cls_lang_features_cls/results.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file must have a 'write' attribute",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m mean_preds \u001B[38;5;241m=\u001B[39m results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean_preds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpreds.p\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: file must have a 'write' attribute"
     ]
    }
   ],
   "source": [
    "mean_preds = results['predictions'].cpu().numpy()\n",
    "pickle.dump(mean_preds, 'preds.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With predictions averaged over all subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 5\n",
    "N_NEIGHBORS = 5\n",
    "# training_mode = 'modality-agnostic'\n",
    "training_mode = 'images'\n",
    "\n",
    "df = data_default_feats.copy()\n",
    "\n",
    "df = df[df.model == MODEL]\n",
    "\n",
    "df = df[df.training_mode == training_mode]\n",
    "\n",
    "# mask = 'whole_brain'\n",
    "# df = df[df['mask'] == mask]\n",
    "df = df[df.surface == False]\n",
    "\n",
    "# features_candidate_set = \"avg\"\n",
    "# df_candidate_set = df[df.features == features_candidate_set].copy()\n",
    "\n",
    "# df = df[df.features == features]\n",
    "\n",
    "assert len(df[df.metric == \"predictions\"]) == len(SUBJECTS)\n",
    "\n",
    "for stimulus_type in [IMAGE, CAPTION]:\n",
    "    print(f\"fMRI stimulus modality: {stimulus_type}\")\n",
    "    all_test_predictions = []\n",
    "    for subject in SUBJECTS:\n",
    "        df_subj = df[df.subject == subject]\n",
    "        \n",
    "        test_predictions = df_subj[df_subj.metric == \"predictions\"].value.item()\n",
    "        test_latents = df_subj[df_subj.metric == \"latents\"].value.item()\n",
    "        test_stimulus_ids = df_subj[df_subj.metric == \"stimulus_ids\"].value.item()\n",
    "        stimulus_types = df_subj[df_subj.metric == \"stimulus_types\"].value.item()\n",
    "    \n",
    "        test_latents = test_latents[stimulus_types == stimulus_type]\n",
    "        test_stimulus_ids = test_stimulus_ids[stimulus_types == stimulus_type]\n",
    "        test_predictions = test_predictions[stimulus_types == stimulus_type]\n",
    "        \n",
    "        # candidate_set_latents = test_latents\n",
    "        # candidate_set_latents_ids = test_stimulus_ids\n",
    "        # ((x - self.mean) / self.std)\n",
    "        # print(test_predictions.mean(axis=1).shape)\n",
    "        # test_predictions = (test_predictions - test_predictions.mean(axis=1).reshape((70, 1)) ) / test_predictions.std(axis=1).reshape((70, 1))\n",
    "        # preds_standardize = Standardize(test_predictions.mean(axis=0), test_predictions.std(axis=0))\n",
    "        # test_predictions = preds_standardize(test_predictions)\n",
    "        all_test_predictions.append(test_predictions)\n",
    "    \n",
    "    \n",
    "    # test_predictions_averaged = np.mean(all_test_predictions, axis=0)\n",
    "    test_predictions_averaged = mean_preds[stimulus_types == stimulus_type]\n",
    "    preds_standardize = Standardize(test_predictions_averaged.mean(axis=0), test_predictions_averaged.std(axis=0)) #TODO\n",
    "    test_predictions_averaged = preds_standardize(test_predictions_averaged)\n",
    "    \n",
    "    candidate_set_latents = np.concatenate((train_latents['sub-01'], test_latents))\n",
    "    candidate_set_latents_ids = np.concatenate((train_stim_ids['sub-01'], test_stimulus_ids))\n",
    "        \n",
    "    dist_mat = get_distance_matrix(test_predictions_averaged, candidate_set_latents)\n",
    "    # acc = dist_mat_to_pairwise_acc(dist_mat)\n",
    "    # print(f\"Pairwise acc: {acc:.2f}\")\n",
    "    np.random.seed(7)\n",
    "    sampled_ids = np.random.choice(range(len(test_stimulus_ids)), NUM_SAMPLES, replace=False)\n",
    "    test_stimulus_ids = test_stimulus_ids[sampled_ids]\n",
    "    dist_mat = dist_mat[sampled_ids]\n",
    "    \n",
    "    for test_stimulus_id, nneighbors_row in zip(test_stimulus_ids, dist_mat):\n",
    "        print(f\"test stimulus: {test_stimulus_id}\")\n",
    "        if stimulus_type == CAPTION:\n",
    "            print(coco_ds.get_stimuli_by_coco_id(test_stimulus_id)[1])\n",
    "        else:\n",
    "            img = coco_ds.get_img_by_coco_id(test_stimulus_id)\n",
    "            new_width  = 400\n",
    "            new_height = round(new_width * img.height / img.width)\n",
    "            display(img.resize((new_width, new_height)))\n",
    "        target_location = np.argwhere(candidate_set_latents_ids == test_stimulus_id)[0][0]\n",
    "        nneighbors_ids = candidate_set_latents_ids[np.argsort(nneighbors_row)]\n",
    "        rank = np.argwhere(nneighbors_ids == test_stimulus_id)[0][0]\n",
    "        # all_ranks.append(rank)\n",
    "        # display(f\"distance to target: {nneighbors_row[target_location]:.3f} | rank: {rank} of {len(nneighbors_row)}\")\n",
    "        # display(f\"nearest neighbors distances: {np.sort(nneighbors_row)[:10]}\")\n",
    "        display_stimuli(nneighbors_ids[:N_NEIGHBORS])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    # print(f\"mean rank: {np.mean(all_ranks)}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SUBJECTS = 2\n",
    "\n",
    "training_mode = 'modality-agnostic'\n",
    "# training_mode = 'images'\n",
    "\n",
    "df = data_default_feats.copy()\n",
    "\n",
    "df = df[df.model == MODEL]\n",
    "df = df[df.training_mode == training_mode]\n",
    "df = df[df.surface == False]\n",
    "\n",
    "assert len(df[df.metric == \"predictions\"]) == len(SUBJECTS)\n",
    "\n",
    "for subject in SUBJECTS[:NUM_SUBJECTS]:\n",
    "    print(f\"\\n\\nSubject: {subject}\")\n",
    "\n",
    "    for stimulus_type in [IMAGE, CAPTION]:\n",
    "        all_ranks = []\n",
    "        print(f\"fMRI stimulus modality: {stimulus_type}\")\n",
    "        df_subj = df[df.subject == subject]\n",
    "        \n",
    "        test_predictions = df_subj[df_subj.metric == \"predictions\"].value.item()\n",
    "        test_latents = df_subj[df_subj.metric == \"latents\"].value.item()\n",
    "        test_stimulus_ids = df_subj[df_subj.metric == \"stimulus_ids\"].value.item()\n",
    "        stimulus_types = df_subj[df_subj.metric == \"stimulus_types\"].value.item()\n",
    "    \n",
    "        test_latents_in_mod = test_latents[stimulus_types == stimulus_type]\n",
    "        test_stimulus_ids_in_mod = test_stimulus_ids[stimulus_types == stimulus_type]\n",
    "        test_predictions_in_mod = test_predictions[stimulus_types == stimulus_type]\n",
    "       \n",
    "        candidate_set_latents = np.concatenate((train_latents[subject], test_latents_in_mod))\n",
    "        candidate_set_latents_ids = np.concatenate((train_stim_ids[subject], test_stimulus_ids_in_mod))\n",
    "    \n",
    "        preds_standardize = Standardize(test_predictions_in_mod.mean(axis=0), test_predictions_in_mod.std(axis=0))\n",
    "        test_predictions_in_mod = preds_standardize(test_predictions_in_mod)\n",
    "    \n",
    "        # targets_standardize = Standardize(candidate_set_latents.mean(axis=0), candidate_set_latents.std(axis=0))\n",
    "        # candidate_set_latents = targets_standardize(candidate_set_latents)\n",
    "    \n",
    "        dist_mat = get_distance_matrix(test_predictions_in_mod, candidate_set_latents) #, metric=\"euclidean\"\n",
    "        acc = dist_mat_to_pairwise_acc(dist_mat)\n",
    "        print(f\"Pairwise acc: {acc:.2f}\")\n",
    "        np.random.seed(7)\n",
    "        sampled_ids = np.random.choice(range(len(test_stimulus_ids_in_mod)), NUM_SAMPLES, replace=False)\n",
    "        test_stimulus_ids_in_mod = test_stimulus_ids_in_mod[sampled_ids]\n",
    "        dist_mat = dist_mat[sampled_ids]\n",
    "        \n",
    "        for test_stimulus_id, nneighbors_row in zip(test_stimulus_ids_in_mod, dist_mat):\n",
    "            print(f\"test stimulus: {test_stimulus_id}\")\n",
    "            if stimulus_type == CAPTION:\n",
    "                print(coco_ds.get_stimuli_by_coco_id(test_stimulus_id)[1])\n",
    "            else:\n",
    "                display(coco_ds.get_img_by_coco_id(test_stimulus_id))\n",
    "            target_location = np.argwhere(candidate_set_latents_ids == test_stimulus_id)[0][0]\n",
    "            nneighbors_ids = candidate_set_latents_ids[np.argsort(nneighbors_row)]\n",
    "            rank = np.argwhere(nneighbors_ids == test_stimulus_id)[0][0]\n",
    "            all_ranks.append(rank)\n",
    "            # display(f\"distance to target: {nneighbors_row[target_location]:.3f} | rank: {rank} of {len(nneighbors_row)}\")\n",
    "            # display(f\"nearest neighbors distances: {np.sort(nneighbors_row)[:10]}\")\n",
    "            display_stimuli(nneighbors_ids[:N_NEIGHBORS])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        # print(f\"mean rank: {np.mean(all_ranks)}\")\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "64c2261fd1335a391d209058834a77a3cc43bbc1dadc63860e2129a303b1f182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
