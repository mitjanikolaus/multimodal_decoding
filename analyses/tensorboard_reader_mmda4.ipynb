{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading tensorboard files for compiling the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T10:31:26.216288860Z",
     "start_time": "2023-12-12T10:31:26.209251348Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import FEATURES_DIR\n",
    "from analyses.ridge_regression_decoding_mni_mmda import NUM_CV_SPLITS\n",
    "\n",
    "plt.style.use('seaborn-v0_8-deep')\n",
    "from glob import glob\n",
    "from glob import escape as gescape\n",
    "from scipy.stats import sem\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T11:34:07.919535982Z",
     "start_time": "2023-12-12T11:34:07.878747470Z"
    }
   },
   "outputs": [],
   "source": [
    "# results_root_dir = os.path.expanduser('~/data/multimodal_decoding/glm/regression_results_mni_mmda4_train')\n",
    "# results_root_dir = os.path.expanduser('~/data/multimodal_decoding/glm/regression_results_mni_mmda_val_set_train')\n",
    "# results_root_dir = os.path.expanduser('~/data/multimodal_decoding/glm/regression_results_mni_mmda_cv_train')\n",
    "results_root_dir = os.path.expanduser('~/data/multimodal_decoding/glm/regression_results_mni_mmda_cv_shuffle_train')\n",
    "\n",
    "model_names = [\n",
    "    'RESNET152_AVGPOOL',\n",
    "   #  'VITL16_ENCODER',\n",
    "   #  'CLIP_V',\n",
    "   # 'BERT_LARGE',\n",
    "   #  'GPT2XL_AVG',\n",
    "   # 'CLIP_L',\n",
    "    # 'RESNET152_AVGPOOL_PCA768',\n",
    "     # 'GPT2XL_AVG_PCA768',\n",
    "     # 'VITL16_ENCODER_PCA768',\n",
    "    # 'CLIP_L_PCA768',\n",
    "    # 'CLIP_V_PCA768'\n",
    "]\n",
    "\n",
    "# subjects = ['sub-01', 'sub-02', 'sub-04', 'sub-05', 'sub-07']\n",
    "subjects = ['sub-01', 'sub-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T11:34:08.393265142Z",
     "start_time": "2023-12-12T11:34:08.371179125Z"
    }
   },
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    def __init__(self, optimizer='SGD', lr=0.01, wd=0.01, dropout=False, loss='MSE', full_train=False) -> None:\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.dropout = dropout\n",
    "        self.loss = loss\n",
    "        self.full_train = full_train\n",
    "    \n",
    "    def get_hp_string(self):\n",
    "        descr = f\"[optim:{self.optimizer}][lr:{str(self.lr).replace('.','-')}][wd:{str(self.wd).replace('.','-')}][drop:{self.dropout}][loss:{self.loss}]\"\n",
    "        if self.full_train:\n",
    "            descr += \"_full_train\"\n",
    "        return descr\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self.optimizer\n",
    "        yield self.lr\n",
    "        yield self.wd\n",
    "        yield self.dropout\n",
    "        yield self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T11:34:08.992049459Z",
     "start_time": "2023-12-12T11:34:08.982340039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.HyperParameters at 0x7f8c35a53d00>,\n",
       " <__main__.HyperParameters at 0x7f8c35a53550>]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXEPOCH = 400\n",
    "\n",
    "test_loss_key = 'Testing/MSE loss'\n",
    "\n",
    "MODE_PYTORCH = \"PYTORCH\"\n",
    "MODE_SKLEARN = \"SKLEARN\"\n",
    "SKLEARN_ALPHAS = [1, 10, 100]\n",
    "\n",
    "MODE = MODE_PYTORCH\n",
    "\n",
    "if MODE == MODE_PYTORCH:\n",
    "    HPs = [\n",
    "        # HyperParameters(optimizer='SGD', lr=0.1, wd=0.0, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='SGD', lr=0.01, wd=0.0, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='SGD', lr=0.001, wd=0.0, dropout=False, loss='MSE'),\n",
    "\n",
    "        # HyperParameters(optimizer='SGD', lr=0.01, wd=0.1, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='SGD', lr=0.01, wd=1, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='SGD', lr=0.01, wd=10, dropout=False, loss='MSE'),\n",
    "\n",
    "        # HyperParameters(optimizer='SGD', lr=0.001, wd=0, dropout=False, loss='MSE', full_train=True),\n",
    "\n",
    "        \n",
    "        # HyperParameters(optimizer='ADAM', lr=0.0001, wd=0.00, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.001, wd=0.00, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.01, wd=0.00, dropout=False, loss='MSE'),\n",
    "\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.0001, wd=0.00, dropout=False, loss='MSE', full_train=True),\n",
    "\n",
    "\n",
    "        # # HyperParameters(optimizer='ADAM', lr=0.001, wd=0.01, dropout=False, loss='MSE'),\n",
    "        # # HyperParameters(optimizer='ADAM', lr=0.010, wd=0.01, dropout=False, loss='MSE'),\n",
    "        # # HyperParameters(optimizer='ADAM', lr=0.1, wd=0.01, dropout=False, loss='MSE'),\n",
    "\n",
    "        HyperParameters(optimizer='ADAM', lr=0.0001, wd=0.1, dropout=False, loss='MSE'),\n",
    "        HyperParameters(optimizer='ADAM', lr=0.001, wd=0.1, dropout=False, loss='MSE'),\n",
    "        # # HyperParameters(optimizer='ADAM', lr=0.01, wd=0.1, dropout=False, loss='MSE'),\n",
    "\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.0001, wd=1, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.001, wd=1, dropout=False, loss='MSE'),\n",
    "        # # HyperParameters(optimizer='ADAM', lr=0.01, wd=1, dropout=False, loss='MSE'),\n",
    "\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.0001, wd=10, dropout=False, loss='MSE'),\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.001, wd=10, dropout=False, loss='MSE'),\n",
    "\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.0001, wd=10, dropout=False, loss='MSE', full_train=True),\n",
    "\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.0001, wd=1, dropout=False, loss='MSE', full_train=True),\n",
    "\n",
    "\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.01, wd=0.01, dropout=False, loss='MSE', full_train=True),\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.01, wd=0.0, dropout=False, loss='MSE', full_train=True),\n",
    "        # HyperParameters(optimizer='ADAM', lr=0.01, wd=0.01, dropout=False, loss='MSE', full_train=True),\n",
    "    ]\n",
    "else:\n",
    "    HPs = [HyperParameters(wd=alpha) for alpha in SKLEARN_ALPHAS]\n",
    "\n",
    "HPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T11:34:09.516253366Z",
     "start_time": "2023-12-12T11:34:09.466070512Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss_results = {}\n",
    "# result_files = {}\n",
    "\n",
    "\n",
    "# for subject in subjects:\n",
    "#     result_files[subject] = {}\n",
    "#     loss_results[subject] = {}\n",
    "#     for model_name in model_names:\n",
    "#         result_files[subject][model_name] = {}\n",
    "#         loss_results[subject][model_name] = {}\n",
    "#         if MODE == MODE_PYTORCH:\n",
    "#             raise NotImplementedError()\n",
    "#         else:\n",
    "#             for alpha, hp in zip(SKLEARN_ALPHAS, HPs):\n",
    "#                 hp_str = f'alpha_{alpha}'\n",
    "#                 distance_matrix_file = os.path.join(results_root_dir, subject, model_name, 'distance_matrix', f'sklearn_alpha_{alpha}', 'distance_matrix.p')\n",
    "#                 print(distance_matrix_file)\n",
    "#                 if os.path.isfile(distance_matrix_file):\n",
    "#                     distance_matrices[subject][model_name][hp] = pickle.load(open(distance_matrix_file, 'rb'))\n",
    "#                 else:\n",
    "#                     distance_matrices[subject][model_name][hp] = None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best decoding accuracy for each HP across training epochs\n",
    "- find the best epoch based on the testing MSE loss\n",
    "- obtain the distance matrix corresponding to that epoch\n",
    "- calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:57:16.497307800Z",
     "start_time": "2023-12-08T13:57:16.494344080Z"
    }
   },
   "outputs": [],
   "source": [
    "# def pairwise_accuracy(dist_mat):\n",
    "#     # dist_mat      # d(i,j) -> distance of the prediction of i to the original of j\n",
    "#     # first 70 -> captions\n",
    "#     # second 70 -> images\n",
    "#     # since the AI model is the same, original of caption is the same as the original of the image\n",
    "    \n",
    "#     diag     = dist_mat.diagonal()[:, np.newaxis]               # all congruent distances\n",
    "#     comp_mat = diag < dist_mat                                  # we are interested in i,j where d(i,i) < d(i,j)\n",
    "#     corrects = comp_mat.sum()                                   # counting the trues (everything is counted two times because of the same ground-truth)\n",
    "    \n",
    "#     n = diag.shape[0]\n",
    "#     score_agnostic = corrects / (n*n-(2*n))                     # -2*n is there to remove the diagonal two times (as it is repeated two times)\n",
    "\n",
    "#     ######\n",
    "#     dist_captions = dist_mat[:70, :70]\n",
    "#     diag     = dist_captions.diagonal()[:, np.newaxis]               \n",
    "#     comp_mat = diag < dist_captions                                  \n",
    "#     corrects = comp_mat.sum()\n",
    "    \n",
    "#     n = diag.shape[0]\n",
    "#     score_captions = corrects / (n*n-n)\n",
    "#     ######\n",
    "#     dist_images   = dist_mat[70:, 70:]\n",
    "#     diag     = dist_images.diagonal()[:, np.newaxis]               \n",
    "#     comp_mat = diag < dist_images                                  \n",
    "#     corrects = comp_mat.sum()\n",
    "    \n",
    "#     n = diag.shape[0]\n",
    "#     score_images = corrects / (n*n-n)\n",
    "\n",
    "#     return score_agnostic, score_captions, score_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:57:17.772289140Z",
     "start_time": "2023-12-08T13:57:17.767947548Z"
    }
   },
   "outputs": [],
   "source": [
    "distance_metric  = ['cosine', 'euclidean'][0]\n",
    "decoding_modes   = ['modality-agnostic', 'captions', 'images', 'rsa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:57:21.579805786Z",
     "start_time": "2023-12-08T13:57:19.488964394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-01\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(len(model_names), len(decoding_modes), figsize=(len(decoding_modes)*6, len(model_names)*4), squeeze=False)\n",
    "xticklabels = subjects[:] + ['average']\n",
    "xticks = np.arange(len(xticklabels))\n",
    "\n",
    "for row, model_name in enumerate(model_names):\n",
    "    # for col, decoding_mode in enumerate(decoding_modes):\n",
    "    bar_width = 0.8/len(HPs)\n",
    "    first_bar_offset = -0.4 + (bar_width/2)\n",
    "    bar_offsets = [first_bar_offset + (i*bar_width) for i in range(len(HPs))]\n",
    "    for hpidx, hp in enumerate(HPs):\n",
    "        subjects_accuracies = {mode: [] for mode in decoding_modes}\n",
    "        for subjectidx, subject in enumerate(subjects):\n",
    "            print(subject)\n",
    "            hp_str = hp.get_hp_string()\n",
    "            if not hp_str.endswith(\"full_train\"):\n",
    "                hp_str += 'fold_0' #TODO\n",
    "            # distance_matrix_file = os.path.join(results_root_dir, subject, model_name, 'distance_matrix', hp_str, 'distance_matrix.p')\n",
    "            results_file_path = os.path.join(results_root_dir, subject, model_name, hp_str, 'results_normalized.p')\n",
    "            if os.path.isfile(results_file_path): \n",
    "                results_file = pickle.load(open(results_file_path, 'rb'))\n",
    "                # accuracies = pairwise_accuracy(result_files[subject][model_name][hp][f\"distance_matrix_{distance_metric}\"])\n",
    "                # print(accuracies)\n",
    "                # print(result_files[subject][model_name][hp][f\"acc_{distance_metric}\"])\n",
    "                subjects_accuracies['modality-agnostic'].append(results_file[f\"acc_{distance_metric}\"])\n",
    "                subjects_accuracies['captions'].append(results_file[f\"acc_{distance_metric}_captions\"])\n",
    "                subjects_accuracies['images'].append(results_file[f\"acc_{distance_metric}_images\"])\n",
    "                rsa = results_file[\"rsa\"]\n",
    "                subjects_accuracies['rsa'].append(rsa)\n",
    "            else:\n",
    "                print(\"not found: \", results_file_path)\n",
    "                subjects_accuracies['modality-agnostic'].append(np.nan)\n",
    "                subjects_accuracies['captions'].append(np.nan)\n",
    "                subjects_accuracies['images'].append(np.nan)\n",
    "                subjects_accuracies['rsa'].append(np.nan)\n",
    "                \n",
    "        for col, decoding_mode in enumerate(decoding_modes):\n",
    "            subjects_accuracies_temp = np.array(subjects_accuracies[decoding_mode])\n",
    "            subjects_mean = subjects_accuracies_temp.mean()\n",
    "            subjects_sem  = [0] * len(xticklabels)\n",
    "            subjects_sem[-1] = sem(subjects_accuracies_temp)\n",
    "            subjects_accuracies[decoding_mode].append(subjects_mean)\n",
    "            axes[row,col].bar(xticks+bar_offsets[hpidx], subjects_accuracies[decoding_mode], bar_width, yerr=subjects_sem, label=f\"{hp.optimizer}(lr:{hp.lr:0.5f}, wd:{hp.wd:0.2f})[{hp.loss}]{'_full_train' if hp.full_train else ''}\")\n",
    "            # axes[row,col].bar(xticks[-1]+bar_offsets[hpidx], subjects_accuracies[-1], yerr=subjects_sem, width=bar_width)\n",
    "            axes[row,col].set_xticks(xticks)\n",
    "            axes[row,col].set_xticklabels(xticklabels)\n",
    "            axes[row,col].grid(alpha=0.5)\n",
    "            axes[row,col].set_title(f'{decoding_mode} | {model_name}')\n",
    "            axes[row,col].set_ylabel(f'Pairwise Accuracy - {distance_metric} distance')\n",
    "            axes[row,col].set_ylim(bottom=0, top=1.0)\n",
    "            # axes[row,col].axhline(y=0.5, color='r', linestyle='-')\n",
    "            if row == 0 and col == len(decoding_modes) - 1:\n",
    "                axes[row,col].legend(bbox_to_anchor=(1.1, 1.05))#loc=\"lower right\")\n",
    "fig.suptitle(\"Test Performance\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"test_performance.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitja/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/mitja/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/mitja/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/mitja/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/mitja/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/mitja/anaconda3/envs/multimodal_decoding/lib/python3.9/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_names = [\"train_loss\", \"val_loss\", \"test_loss\"]\n",
    "fig, axes = plt.subplots(len(model_names), len(loss_names), figsize=(len(loss_names)*6, len(model_names)*4), squeeze=False)\n",
    "xticklabels = subjects[:] + ['average']\n",
    "xticks = np.arange(len(xticklabels))\n",
    "\n",
    "current_hps = [hp for hp in HPs if not hp.full_train]\n",
    "for row, model_name in enumerate(model_names):\n",
    "    # for col, decoding_mode in enumerate(decoding_modes):\n",
    "    bar_width = 0.8/len(current_hps)\n",
    "    first_bar_offset = -0.4 + (bar_width/2)\n",
    "    bar_offsets = [first_bar_offset + (i*bar_width) for i in range(len(current_hps))]\n",
    "    for hpidx, hp in enumerate(current_hps):\n",
    "        subjects_losses = {name: [] for name in loss_names}\n",
    "        for subjectidx, subject in enumerate(subjects):\n",
    "            losses = {name: [] for name in loss_names}\n",
    "            for fold in range(NUM_CV_SPLITS):\n",
    "                hp_str = hp.get_hp_string() + f\"fold_{fold}\"\n",
    "                \n",
    "                loss_results_file = os.path.join(results_root_dir, subject, model_name, 'loss_results', hp_str, 'loss_results.p')\n",
    "                if os.path.isfile(loss_results_file):\n",
    "                    loss_results = pickle.load(open(loss_results_file, 'rb'))\n",
    "                    for name in loss_names:\n",
    "                        losses[name].append(loss_results[name])\n",
    "                else:\n",
    "                    for name in loss_names:\n",
    "                        losses[name].append(np.nan)\n",
    "            for name in loss_names:\n",
    "                if name in loss_results.keys():\n",
    "                    subjects_losses[name].append(np.mean(losses[name]))\n",
    "                else:\n",
    "                    subjects_losses[name].append(np.nan)\n",
    "        for col, loss_name in enumerate(loss_names):\n",
    "            subjects_losses_temp = np.array(subjects_losses[loss_name])\n",
    "            subjects_mean = subjects_losses_temp.mean()\n",
    "            subjects_sem  = [0] * len(xticklabels)\n",
    "            subjects_sem[-1] = sem(subjects_losses_temp)\n",
    "            subjects_losses[loss_name].append(subjects_mean)\n",
    "            axes[row,col].bar(xticks+bar_offsets[hpidx], subjects_losses[loss_name], bar_width, yerr=subjects_sem, label=f\"{hp.optimizer}(lr:{hp.lr:0.5f}, wd:{hp.wd:0.2f})[{hp.loss}]{'_full_train' if hp.full_train else ''}\")\n",
    "            # axes[row,col].bar(xticks[-1]+bar_offsets[hpidx], subjects_accuracies[-1], yerr=subjects_sem, width=bar_width)\n",
    "            axes[row,col].set_xticks(xticks)\n",
    "            axes[row,col].set_xticklabels(xticklabels)\n",
    "            axes[row,col].grid(alpha=0.5)\n",
    "            axes[row,col].set_title(f'{loss_name} | {model_name}')\n",
    "            axes[row,col].set_ylabel(f'{loss_name}')\n",
    "            if loss_name == \"train_loss\":\n",
    "                axes[row,col].set_ylim(bottom=0, top=2)\n",
    "            if loss_name == \"val_loss\":\n",
    "                axes[row,col].set_ylim(bottom=0.5, top=2)\n",
    "            if loss_name == \"test_loss\":\n",
    "                axes[row,col].set_ylim(bottom=0.5, top=2)\n",
    "            # axes[row,col].axhline(y=0.5, color='r', linestyle='-')\n",
    "            if row == 0 and col == len(decoding_modes) - 1:\n",
    "                axes[row,col].legend(bbox_to_anchor=(1.1, 1.05))\n",
    "fig.suptitle(\"Losses\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"losses.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:57:34.136197757Z",
     "start_time": "2023-12-08T13:57:31.678798003Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(len(HPs), len(decoding_modes), figsize=(len(decoding_modes)*6, len(HPs)*4), squeeze=False)\n",
    "# xticklabels = subjects[:] + ['average']\n",
    "# xticks = np.arange(len(xticklabels))\n",
    "\n",
    "# for row, hp in enumerate(HPs):\n",
    "#     # for col, decoding_mode in enumerate(decoding_modes):\n",
    "#         bar_width = 0.8/len(model_names)\n",
    "#         first_bar_offset = -0.4 + (bar_width/2)\n",
    "#         bar_offsets = [first_bar_offset + (i*bar_width) for i in range(len(model_names))]\n",
    "#         for model_idx, model_name in enumerate(model_names):\n",
    "#             subjects_losses = {'modality-agnostic':[], 'captions':[], 'images':[]}\n",
    "#             for subjectidx, subject in enumerate(subjects):\n",
    "#                 if distance_matrices[subject][model_name][hp] is not None:\n",
    "#                     accuracies = pairwise_accuracy(distance_matrices[subject][model_name][hp][distance_metric])\n",
    "#                     # accuracies = pairwise_accuracy_duo(distance_matrices[subject][model_name][hp][best_epoch][distance_metric], distance_matrices[subject][model_name][hp][best_epoch]['classes'])\n",
    "#                     subjects_losses['modality-agnostic'].append(accuracies[0])\n",
    "#                     subjects_accuracies['captions'].append(accuracies[1])\n",
    "#                     subjects_accuracies['images'].append(accuracies[2])\n",
    "#                 else:\n",
    "#                     subjects_accuracies['modality-agnostic'].append(np.nan)\n",
    "#                     subjects_accuracies['captions'].append(np.nan)\n",
    "#                     subjects_accuracies['images'].append(np.nan)\n",
    "#             for col, decoding_mode in enumerate(decoding_modes):\n",
    "#                 subjects_accuracies_temp = np.array(subjects_accuracies[decoding_mode])\n",
    "#                 subjects_mean = subjects_accuracies_temp.mean()\n",
    "#                 subjects_sem  = [0] * len(xticklabels)\n",
    "#                 subjects_sem[-1] = sem(subjects_accuracies_temp)\n",
    "#                 subjects_accuracies[decoding_mode].append(subjects_mean)\n",
    "#                 axes[row,col].bar(xticks+bar_offsets[model_idx], subjects_accuracies[decoding_mode], bar_width, yerr=subjects_sem, label=f'{model_name}')\n",
    "#                 axes[row,col].set_xticks(xticks)\n",
    "#                 axes[row,col].set_xticklabels(xticklabels)\n",
    "#                 axes[row,col].grid(alpha=0.5)\n",
    "#                 axes[row,col].set_title(f\"{decoding_mode} | {hp.optimizer}(lr:{hp.lr:0.2f}, wd:{hp.wd:0.2f})[{hp.loss}]{'_full_train' if hp.full_train else ''}\")\n",
    "#                 axes[row,col].set_ylim(bottom=0.45, top=1.0)\n",
    "#                 axes[row,col].set_ylabel(f'Pairwise Accuracy - {distance_metric} Distance')\n",
    "#                 if row == 0 and col == len(decoding_modes) - 1:\n",
    "#                     axes[row,col].legend(bbox_to_anchor=(1.1, 1.05))\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'pairwise_acc_{distance_metric}.jpg', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imagery decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:57:42.767164960Z",
     "start_time": "2023-12-08T13:57:42.760058033Z"
    }
   },
   "outputs": [],
   "source": [
    "# load subjects best module\n",
    "# load imagery beta files\n",
    "# load all latent vectors\n",
    "# search for the nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:46.618280281Z",
     "start_time": "2023-12-08T13:58:46.566887765Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import IMAGERY_SCENES\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-deep')\n",
    "from glob import glob\n",
    "from glob import escape as gescape\n",
    "from ridge_regression_decoding_mni_mmda import HyperParameters, LinearNet, COCOBOLDDataset\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.transforms import Compose\n",
    "import nibabel as nib\n",
    "\n",
    "def get_nearest_neighbors_indices(vector, dataset_vectors, n_neighbors, metric='cosine'):\n",
    "    r\"\"\"\n",
    "    extracts neighboring vectors of each given vector with respect to the dataset.\n",
    "    metric defines the distance metric for finding neighbors\n",
    "    \"\"\"\n",
    "    # if vector is 2-dim, the neighbors will be returned for each sample\n",
    "    if np.ndim(vector) == 1:\n",
    "        vector = vector[np.newaxis,:]\n",
    "    \n",
    "    dists = cdist(vector, dataset_vectors, metric=metric)\n",
    "    nearests_ids   = np.argsort(dists, axis=1)[:, :n_neighbors]\n",
    "    nearests_dists = np.sort(dists, axis=1)[:, :n_neighbors]\n",
    "\n",
    "    return nearests_ids, nearests_dists\n",
    "\n",
    "def fetch_image(sid, ds_root):\n",
    "    file_name = f\"{sid:012d}.jpg\"\n",
    "    image_add = list(glob(os.path.join(ds_root, '*', file_name)))\n",
    "    return plt.imread(image_add[0])\n",
    "\n",
    "def show_neighboring_images(neighboring_ids, distances, imagery_scene):\n",
    "    rows = int(np.ceil(len(neighboring_ids)/10))\n",
    "    fig, axes = plt.subplots(rows, 10, figsize=(30,rows*3), facecolor='white', squeeze=False)\n",
    "    fig.suptitle(imagery_scene, fontsize=20)\n",
    "    row = 0\n",
    "    col = 0\n",
    "    for c in range(len(neighboring_ids)):\n",
    "        axes[row,col].imshow(fetch_image(neighboring_ids[c], '/home/milad/datasets/coco2017/images'))\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(f\"{distances[c]:.4f}\")\n",
    "        col += 1\n",
    "        if col == 10:\n",
    "            row += 1\n",
    "            col = 0\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig('temp.jpg', dpi=96)\n",
    "    plt.close()\n",
    "\n",
    "def decode_imagery(net, imagery_loader, device):\n",
    "    net.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in imagery_loader:\n",
    "            test_inputs, _,_, test_ids = data\n",
    "            outputs = net(test_inputs.to(device))\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "    return predictions\n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, x):\n",
    "        return ((x-self.mean)/self.std).astype(np.float32).squeeze()\n",
    "\n",
    "def to_tensor(v):\n",
    "    return torch.from_numpy(v)\n",
    "\n",
    "def normalize_vectors(latent_vectors, normalizer):\n",
    "    nvects = []\n",
    "    for v in latent_vectors:\n",
    "        nvects.append(normalizer(v).numpy())\n",
    "    return np.array(nvects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:47.933007102Z",
     "start_time": "2023-12-08T13:58:47.923587068Z"
    }
   },
   "outputs": [],
   "source": [
    "# subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-07']\n",
    "# model_names = ['ViT-L14-336px','ViTL16_GPT2XL', 'RESNET152_GPT2XL']\n",
    "# latent_vector_files = {\n",
    "#     \"RESNET152_AVGPOOL_PCA768\" :os.path.join(FEATURES_DIR, \"resnet/resnet152_avgpool_selected_coco_crop_pca_768.pickle\"),\n",
    "#     # \"RESNET152_AVGPOOL\"        :os.path.join(FEATURES_DIR, \"resnet/resnet152_avgpool_selected_coco_crop.pickle\"),\n",
    "#     \"GPT2XL_AVG_PCA768\"        :os.path.join(FEATURES_DIR, \"gpt/gpt2_xl_avg_selected_coco_pca_768.pickle\"),\n",
    "#     \"GPT2XL_AVG\"               :os.path.join(FEATURES_DIR, \"gpt/gpt2_xl_avg_selected_coco.pickle\"),\n",
    "#     \"VITL16_ENCODER_PCA768\"    :os.path.join(FEATURES_DIR, \"vit/vit_l_16_encoder_selected_coco_crop_pca_768.pickle\"),\n",
    "#     \"VITL16_ENCODER\"           :os.path.join(FEATURES_DIR, \"vit/vit_l_16_encoder_selected_coco_crop.pickle\"),\n",
    "#     \"CLIP_L_PCA768\"            :os.path.join(FEATURES_DIR, \"clip/clip_l_VITL14336px_selected_coco_dataset_crop_pca_768.pickle\"),\n",
    "#     \"CLIP_L\"                   :os.path.join(FEATURES_DIR, \"clip/clip_l_VITL14336px_selected_coco_dataset_crop.pickle\"),\n",
    "#     \"CLIP_V_PCA768\"            :os.path.join(FEATURES_DIR, \"clip/clip_v_VITL14336px_selected_coco_dataset_crop_pca_768.pickle\"),\n",
    "#     \"CLIP_V\"                   :os.path.join(FEATURES_DIR, \"clip/clip_v_VITL14336px_selected_coco_dataset_crop.pickle\"),\n",
    "# }\n",
    "\n",
    "# two_stage_glm_dir   = f'/mnt/HD1/milad/multimodal_decoding/glm_manual/two-stage-mni/'\n",
    "# bold_std_mean_name  = f'bold_mean_std'\n",
    "# netowrks_base_dir   = f'/mnt/HD1/milad/multimodal_decoding/regression_results_mni'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:49.003323676Z",
     "start_time": "2023-12-08T13:58:48.983868226Z"
    }
   },
   "outputs": [],
   "source": [
    "HPs = [\n",
    "    HyperParameters(optimizer='SGD', lr=0.010, wd=0.00, dropout=False, loss='MSE'),\n",
    "    # HyperParameters(optimizer='SGD', lr=0.050, wd=0.00, dropout=False, loss='MSE'),\n",
    "    # HyperParameters(optimizer='SGD', lr=0.010, wd=0.01, dropout=False, loss='MSE'),\n",
    "    # HyperParameters(optimizer='SGD', lr=0.050, wd=0.01, dropout=False, loss='MSE'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding images from imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:51.115948916Z",
     "start_time": "2023-12-08T13:58:51.108255108Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_imagery_vision_to_vision_decoder(subject, model_name, hp, netowrks_base_dir):\n",
    "    model_std_mean_name = f'{model_name}_mean_std'\n",
    "    std_mean_dir        = f'/mnt/HD1/milad/multimodal_decoding/glm_manual/two-stage-mni/{subject}'\n",
    "\n",
    "    # preparing the data transforms\n",
    "    with open(latent_vector_files[model_name], 'rb') as handle:\n",
    "        latent_vectors = pickle.load(handle)\n",
    "\n",
    "    stim_ids = list(sorted(latent_vectors.keys()))\n",
    "    vision_vectors   = [latent_vectors[sid]['visual_feature'] for sid in stim_ids]\n",
    "    # language_vectors = [latent_vectors[sid]['lingual_feature'] for sid in stim_ids]\n",
    "\n",
    "    # bold images\n",
    "    with open(os.path.join(std_mean_dir, f'{bold_std_mean_name}_train_images.pickle'), 'rb') as handle:\n",
    "        bold_mean_std = pickle.load(handle)\n",
    "    bold_images_transform = Compose([\n",
    "        Normalize(bold_mean_std['mean'], bold_mean_std['std']),\n",
    "        to_tensor\n",
    "    ])\n",
    "\n",
    "    # # bold captions\n",
    "    # with open(os.path.join(std_mean_dir, f'{bold_std_mean_name}_train_captions.pickle'), 'rb') as handle:\n",
    "    #     bold_mean_std = pickle.load(handle)\n",
    "    # bold_captions_transform = Compose([\n",
    "    #     Normalize(bold_mean_std['mean'], bold_mean_std['std']),\n",
    "    #     to_tensor\n",
    "    # ])\n",
    "\n",
    "    # latent images\n",
    "    with open(os.path.join(std_mean_dir, f'{model_std_mean_name}_train_images.pickle'), 'rb') as handle:\n",
    "        model_mean_std = pickle.load(handle)\n",
    "    latent_images_transform = Compose([\n",
    "        Normalize(model_mean_std['mean'], model_mean_std['std']),\n",
    "        to_tensor\n",
    "    ])\n",
    "\n",
    "    # # latent captions\n",
    "    # with open(os.path.join(std_mean_dir, f'{model_std_mean_name}_train_captions.pickle'), 'rb') as handle:\n",
    "    #     model_mean_std = pickle.load(handle)\n",
    "    # latent_captions_transform = Compose([\n",
    "    #     Normalize(model_mean_std['mean'], model_mean_std['std']),\n",
    "    #     to_tensor\n",
    "    # ])\n",
    "\n",
    "\n",
    "    imagery_to_vision_dataset = COCOBOLDDataset(two_stage_glm_dir, subject, latent_vectors, f'imagery', transform=bold_images_transform)\n",
    "    imagery_to_vision_dataset.preload()\n",
    "    imagery_to_vision_loader  = DataLoader(imagery_to_vision_dataset,  batch_size=len(imagery_to_vision_dataset), num_workers=0, shuffle=False)\n",
    "\n",
    "    # imagery_to_captions_dataset = COCOBOLDDataset(two_stage_glm_dir, subjects[0], latent_vectors, f'imagery', transform=bold_captions_transform)\n",
    "    # imagery_to_captions_dataset.preload()\n",
    "    # imagery_to_captions_loader  = DataLoader(imagery_to_captions_dataset,  batch_size=len(imagery_to_captions_dataset), num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "    checkpoint = list(glob(os.path.join(gescape(os.path.join(netowrks_base_dir, subject, model_name, 'networks', hp.get_hp_string())), f'net_best_vision_vision*')))[0]\n",
    "    net_vision = LinearNet(imagery_to_vision_loader.dataset.bold_dim_size, imagery_to_vision_loader.dataset.latent_dim_size)\n",
    "    net_vision.load_state_dict(torch.load(checkpoint, map_location='cpu'))\n",
    "\n",
    "    predictions = decode_imagery(net_vision, imagery_to_vision_loader, 'cpu')\n",
    "\n",
    "    normalized_vision_vectors = normalize_vectors(vision_vectors, latent_images_transform)\n",
    "\n",
    "    neighbors, distances = get_nearest_neighbors_indices(predictions-predictions.mean(axis=0), normalized_vision_vectors, 20)\n",
    "    # neighbors, distances = get_nearest_neighbors_indices(predictions, normalized_vision_vectors, 20)\n",
    "\n",
    "    for i in range(3):\n",
    "        show_neighboring_images([stim_ids[a] for a in neighbors[i]], distances[i], IMAGERY_SCENES[subject][i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:52.811222405Z",
     "start_time": "2023-12-08T13:58:52.744673651Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ViT-L14-336px'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecode_imagery_vision_to_vision_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHPs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetowrks_base_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m, in \u001b[0;36mdecode_imagery_vision_to_vision_decoder\u001b[0;34m(subject, model_name, hp, netowrks_base_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m std_mean_dir        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/HD1/milad/multimodal_decoding/glm_manual/two-stage-mni/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# preparing the data transforms\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mlatent_vector_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m      7\u001b[0m     latent_vectors \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(handle)\n\u001b[1;32m      9\u001b[0m stim_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28msorted\u001b[39m(latent_vectors\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ViT-L14-336px'"
     ]
    }
   ],
   "source": [
    "decode_imagery_vision_to_vision_decoder(subjects[0], model_names[0], HPs[0], netowrks_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:54.368124849Z",
     "start_time": "2023-12-08T13:58:54.330891027Z"
    }
   },
   "outputs": [],
   "source": [
    "decode_imagery_vision_to_vision_decoder(subjects[1], model_names[0], HPs[0], netowrks_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:54.964512464Z",
     "start_time": "2023-12-08T13:58:54.945731008Z"
    }
   },
   "outputs": [],
   "source": [
    "decode_imagery_vision_to_vision_decoder(subjects[3], model_names[0], HPs[0], netowrks_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:55.476736715Z",
     "start_time": "2023-12-08T13:58:55.444019157Z"
    }
   },
   "outputs": [],
   "source": [
    "decode_imagery_vision_to_vision_decoder(subjects[4], model_names[0], HPs[0], netowrks_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:58:55.974826665Z",
     "start_time": "2023-12-08T13:58:55.952170647Z"
    }
   },
   "outputs": [],
   "source": [
    "decode_imagery_vision_to_vision_decoder(subjects[5], model_names[0], HPs[0], netowrks_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "64c2261fd1335a391d209058834a77a3cc43bbc1dadc63860e2129a303b1f182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
